\chapter{Canonical Correlation Analysis for Data Fusion}
\label{chap:CCA}
\chaptermark{Canonical Correlation Analysis for Data Fusion}

\section{Canonical Correlational Analysis on \\ Multidimensional Scaling embeddings\label{sec:CCA}}

Canonical Correlational Analysis is another method for addressing the incommensurability of dissimilarities from different conditions. We will refer to the match detection task in \autoref{chap:match_detection} and the data settings in \autoref{sec:data_settings} to explain this alternative approach.

For the CCA approach, MDS is used to compute embedding configurations, $ \X_1$ and $\X_2$ from the disparate dissimilarity matrices $\Delta_1$ and $\Delta_2$. For the data settings in  \autoref{sec:data_settings}, it is desirable to perform the embedding into the highest-possible dimensional space  ($\mathbb{R}^{s}$, where $s=p+q$ for the Gaussian and Dirichlet settings)  to  preserve as many of the signal dimensions as possible (at the risk of possibly including  some noise dimensions). CCA~\cite{Hardoon2004}, then,  yields two mappings $\mathcal{U}_1$ and $\mathcal{U}_2$ that map these embeddings in $\mathbb{R}^{s}$ to  the low-dimensional commensurate space ($\mathbb{R}^d$). 

While embedding the dissimilarities in the  highest dimension possible is a good idea for preserving the signal dimensions, in the presence of noise dimensions (\ref{subsec:noise}), the noise will be incorporated into the embeddings. Even if the dissimilarities are errorless representations of measurements of a particular dimension $d^*$, for the  sake of inference, it is preferable to embed at a lower dimension $d<{d^*}$ because of  the bias-variance tradeoff. We call this variant of the CCA approach regularized CCA. Regularized CCA, for which the embedding dimension choice is  $s$ such that $d < s < (p+q)$, is expected to yield a better performance than the CCA approach by introducing more bias for the sake of removing variance. We expect to see a difference between CCA and regularized CCA  in our data settings because we introduce noise into the dissimilarities \ref{chap:data_models}.

\section{Canonical Correlational Analysis\label{sec:CCA}}

 Let $X$ and $Y$ be two $s$-dimensional random vectors. If we want to find  the pair of linear projection operators $U_1:\mathbb{R}^s \rightarrow  \mathbb{R}$, $U_2 :\mathbb{R}^s \rightarrow  \mathbb{R}$ that maximize the correlation between the projections of $X$ and $Y$, CCA provides the solution to this problem by optimizing the objective function
 $$
({\hat{(u)}_1 ,\hat{(u)}_2})=\arg\max_{u_1\in\mathbb{R}^s,u_2\in\mathbb{R}^s} {\frac{E[u_1^{T}XY^Tu_2]}{{E[u_1^{T}XX^T u_1]}{E[u_2^{T}YY^T u_2]}}}$$
with the constraints $E[{u_1^{T}XX^T u_1}]=1 , E[{u_2^{T}YY^T u_2}]=1$ for removing ambiguities. The constraints simplify the objective function to $$
\max_{u_1\in \mathbb{R}^s,u_2\in \mathbb{R}^s} {E[u_1^{T}XY^Tu_2]}.$$ Then, the projection operators are $U_1(x)=(\hat{u}_1)^{T} x$ and $U_1(y)=(\hat{u}_2)^{T} y$.
\begin{remark}
$X$ and $Y$ can be of different dimensions $s_1$ and $s_2$, but for  the sake of simplicity, we will assume they have the same dimension $s$. 
\end{remark}
In general, the projections map to a pair of $d$-dimensional linear subspaces, that is, $U_1:\mathbb{R}^s \rightarrow  \mathbb{R}^{d}$, $U_2 :\mathbb{R}^s \rightarrow  \mathbb{R}^{d}$. The projection matrices that represent the mappings are $\mathcal{U}_1$ and $\mathcal{U}_2$,  and their rows are the direction vectors ${{u}_{1(i)},{u}_{2(i)}}, i=1,\ldots,d $. These additional pairs of projection vectors can be computed sequentially, with the constraints that the projections along the new directions are uncorrelated with the projections along previous directions. That is, the $i^{th}$ pair of directions  that maximize correlation is computed by 
$$
{\hat{(u)}_{1(i)},\hat{(u)}_{2(i)}}=\arg\max_{u_{1(i)},u_{2(i)}\in\mathbb{R}^s} {E[u_{1(i)}^{T}XY^Tu_{2(i)}]}$$ subject to constraints $E[{u_{1(i)}^{T}XX^T u_{1(i)}}]=1$, $E[{u_{2(i)}^{T}YY^T u_{2(i)}}]=1$, $E[{u_{1(i)}^{T}XX^T u_{1(j)}}]=0$,  
   $ E[{u_{2(i)}^{T}YY^T u_{2(j)}}]=\nolinebreak0$ $\forall \quad  j=1,\ldots,i-1$.  These directions are called ``canonical'' directions. The projections $\X \hat{(u)}_{1(i)}, i=1,\ldots,d$ are called ``canonical'' variates.
   
   For sample CCA, $E[XX^T]$,$E[YY^T]$ and $E[XY^T]$ are replaced with their sample estimates.


Note that $s$, the dimension of $X$ and $Y$, is the embedding dimension  $s$ we use in the CCA approach. So we use  MDS to separately embed  the dissimilarities in $\Delta_1$ and $\Delta_2$ in ${\R}^s$, and  then use CCA to project the embeddings to the $d$-dimensional Euclidean space. CCA$\circ$MDS provides the complete mapping from dissimilarities to the commensurate space.


As in P$ \circ $M, new dissimilarities are OOS embedded and mapped to a commensurate  space by maps provided by CCA. The test statistic $\tau$, which is the distance between the points in the commensurate space that correspond to the OOS dissimilarities,  can now be computed, and the null hypothesis is rejected for ``large'' values of the test statistic $\tau$, as in Section \ref{chap:PoM}.



\section{Geometric Interpretation of \\ Canonical Correlational Analysis}

To complement CCA, we should also consider Canonical Variate Analysis (CVA). In CVA, the  projections are also maximally correlated; however, one is concerned with the variates $a_i={u}_{1(i)}^TX $ and $b_i={u}_{2(i)}^T Y $, in contrast to the canonical directions ${u}_{1(i)}$ and ${u}_{2(i)}$.  CVA is to CCA  what Principal Coordinate Analysis is to Principal Component Analysis.

We should also define canonical angles as follows:

\begin{defn}
For two subspaces $\mcV$ and $\mcW$ of $\R^d$, the first \emph{canonical} (or \emph{principal})  angle  between them  is $\arccos \max_{v \in \mcV, w \in \mcW}\frac{<v,w>}{\|v\|\|w\|}$. Other ($i^{th}$) canonical angles are defined as 
$\arccos \max_{v_i \in \mcV, w_i \in \mcW, v_i \perp v_j, w_i \perp w_j, \forall j<i}\frac{<v_i,w_i>}{\|v_i\|\|w_i\|}$.
The vectors $v_i$ and $w_i$ that maximize $\frac{<v_i,w_i>}{{\|v_i\|}{\|w_i\|}}$ are called \emph{canonical} vectors.
\end{defn}

For two $n \times s$ configuration matrices $\X$ and $\Y$, consider the column spaces of the two matrices $\mcL{\X}=\{\X u : u \in \R^s\}$ and ${\mcL{\Y}}$. Note that these spaces are subspaces of $\R^n$, not of $\R^s$ (the $n$ points of the configuration lie in $\R^s$). 
We already know that CCA/CVA maximizes the correlation of the variates. Let us we borrow terminology from pattern recognition and call any one-dimensional subspace of $\mcL{\X}$ (and $\mcL{\Y}$) a ``feature''. Each $u \in \R^s$ define  a feature. Therefore, any linear combination of the the original feature vectors (rows of $\X$ and $\Y$) is also a feature. The (sample) correlation of the variates (defined by the canonical directions $u_{1(i)}$ and $u_{2(i)}$) in CCA is also the cosine of the angle between the features defined by the same directions. The uncorrelatedness condition of two canonical variates of $\X$  correspond to the perpendicularity of the corresponding feature vectors of $\X$. Thus,
 CCA/CVA for $d$ variates solves the problem of finding the first $d$ canonical angles and the corresponding canonical directions of $\mcL{\X}$ and $\mcL{\Y}$.
 



\section[Relationship between CCA and Commensurability]{Relationship between \\ Canonical  Correlational Analysis and \\ Commensurability} 

\begin{thm}

Let $\X_1$ and $\X_2$ be two $n\times s$ (configuration) matrices that represent pairs of points that are perfectly ``matched''
 (there exists a  matrix $\mathbf{Q}$ such that $\|   \X_1\mathbf{Q}  -\X_2 \|=0$).
 Suppose, for the joint embedding procedure, that the embedded configurations are constrained to be of the form $\widetilde{\X_1}=\X_1U_1$ and $\widetilde{\X_2}=\X_2U_2$ for some  $U_1\in \mathcal{U}$ and $U_2\in  \mathcal{U}$, where  $ \mathcal{U}$ be the set of all orthogonal $d$-frames 
(ordered set of $d$ linearly independent vectors) of $\R^{s}$. Elements of  $ \mathcal{U}$  correspond to the unique projection operators to $d$-dimensional linear subspaces of  $\R^{s}$.
%and   the original dissimilarities are $D(\X_1)$ and $D(\X_2) $,
The commensurability error is  defined as it is
in equation~\eqref{comm-error}.
 
 Canonical Correlational Analysis on the  \emph{i.i.d.} sample of points represented by  $\X_1$ and $\X_2$ gives $\mathbf{U}_1\in\mathcal{U}$ and  $\mathbf{U}_2\in\mathcal{U}$, 
 the two elements of $\mathcal{U}$ that maximize commensurability, subject to $U_1^{T}\X_1^{T}\X_1U_1=I_d$, and $U_2^{T}\X_2^{T}\X_2U_2=I_d$ ($I_d$ is the $d \times d$ identity matrix).
\end{thm}

\begin{proof}
 Consider \ref{comm-error} and its simplified form when $\delta_{ij}=0$, as we assumed that there exists a perfect matching. 
 %For defining commensurability error, let us use expectation of the squared error term instead of the average squared error from $n$ pairs of points. Then, commensurability error subject to the linearity constraint is  
 %\[
%\epsilon_{c_{(k_1=1,k_2=2)}} = E \left[d(U_x\ {\bm{x}}_{k_1},U_y \ {\bm{x}}_{k_2})\right]^2
%\label{eq:cca_commens_proof_1}.\]
 \begin{equation}
 \epsilon_{c_{(k_1=1,k_2=2)}} = \frac{1}{n} \sum_{1 \leq i \leq n;k_1=1,k_2=2} (d(U_1{\bm{x}}_{ik_1},U_2 {\bm{x}}_{ik_2}))^2.
\label{eq:cca_commens_proof_1}
\end{equation}
Equation \eqref{eq:cca_commens_proof_1} can be written as
%\begin{align*}
%\epsilon_{c12} &=  \sum_{j=1}^d E \left[((u_{xj}{\bm{x}}_{1}-u_{yj} {\bm{x}}_{2}))\right]^2 \\
%&=  \sum_{j=1}^d {(u_{xj}\ {\bm{x}}_{i1}})^2+ ( u_{yj} \ {\bm{x}}_{i2})^2 - 2 (u_{xj}  {\bm{x}}_{i1} u_{yj}  {\bm{x}}_{i2}).
%\end{align*}

\begin{align*}
\epsilon_{c12} &=  \frac{1}{n}  \sum_{j=1}^d \sum_{i=1}^n \left[((u_{j1}{\bm{x}}_{i1}-u_{j2} {\bm{x}}_{i2}))\right]^2 \\
&=  \frac{1}{n}  \sum_{j=1}^d \sum_{i=1}^n {(u_{j1}\ {\bm{x}}_{i1}})^2+ ( u_{j2} \ {\bm{x}}_{i2})^2 - 2 (u_{j1}  {\bm{x}}_{i1} u_{j2}  {\bm{x}}_{i2}),
\end{align*}
where ${u_{j1}}$ and ${u_{j2}}$ are the rows of $U_1$ and $U_2$.

Because $U_1^{T}\X_1^{T}\X_1U_1=I_d$ (and $U_2^{T}\X_2^{T}\X_2U_2=I_d$), for any $j$, ${u_{j1}}\X_1^{T}\X_1{u_{j1}^T}=1$ (${u_{j2}}\X_2^{T}\X_2{u_{j2}^T}=1$ ). 

Consider the sum of the first  terms, $S_1=\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{d} {(u_{j1} {\bm{x}}_{i1}})^2  $. %${u_{j1}}\X_1^{T}\X_1{u_{j1}^T}=1$
It is easier to show this sum is constant, if we use the the probabilistic definition of CCA in \autoref{sec:CCA}  . Assume $\X_1$ and $\X_2$ represent $n$-sized sample of $X$ and $Y$, respectively.  As $n \rightarrow \infty$, the sum $S_1=\sum_{j=1}^{d} \frac{1}{n} \sum_{i=1}^{n} {(u_{j1} {\bm{x}}_{i1}})^2  $  converges to $\sum_{j=1}^{d} E[u_{j1}X ]$. Each term of this limit sum is   constrained to be 1  in the definition of CCA \ref{sec:CCA}. Therefore the sum $S_1$ converges to $d$.  By the same line of reasoning, we can conclude the sum of the second terms is also constant.
%(${(u_{j2} {\bm{x}}_{i2}})^2 $ )
 The sum of the  third terms can be written in the form of  ($ -2 \times \xi$), where $\xi$ is the sum of the products of dot products  $u_{j1}  {\bm{x}}_{i1}$ and  $u_{j2}  {\bm{x}}_{i2}$. Thus, maximizing $\xi$ under the linearity constraints  is maximizing  the commensurability.


Note that 
\begin{align*}
\xi &=  \frac{1}{n}  \sum_{i=1}^n \sum_{j=1}^d (u_{j1}  {\bm{x}}_{i1} u_{j2}  {\bm{x}}_{i2}) = \frac{1}{n} \left[ \tr{U_1^{T}\X_1^{T}\X_2U_2} - \sum_{1 \leq j \leq d} \sum_{1 \leq i < l \leq d} u_{j1}  {\bm{x}}_{i1}  {\bm{x}}_{l2}^T u_{j2}^T \right]
\end{align*}
where the dot products $u_{j1}^T  {\bm{x}}_{i1}$ and  $ {\bm{x}}_{l2}^T u_{j2}$  are uncorrelated because ${\bm{x}}_{i1},{\bm{x}}_{l2}, i \neq l$  are independent samples. Therefore, as $n \rightarrow \infty$, the second sum vanishes, and only the trace term remains.  
Thus, $\xi= \frac{1}{n}\tr{U_1^{T}\X_1^{T}\X_2U_2}$, which is the objective function optimized with respect to $U_1$ and $U_2$ in the canonical correlational analysis. Subject to constraints, CCA maximizes commensurability  with respect to $U_1$ and $U_2$.

\end{proof}


\section{Spectral Embedding Generalization of CCA}

Another way to view the connection between CCA and JOFC embedding (using classical MDS) is via connections to spectral embedding. Jagarlamudi et al.\cite{CCAviaSpectralEmbed} show that CCA is a special case of Spectral Embedding with the restriction that the joint embedding coordinates are linear projections of the original multiview data, $\X_1$ and $\X_2$. First, we define ``Spectral Embedding'' as follows:
Given  a $k \times k$ weight matrix $W$, Spectral Embedding embeds $k$ points in $d$-dimensional Euclidean space by  minimizing the cost function $\sum_{i,j \in \{1,\ldots,k\}}W_{ij}\left(u_i-u_j\right)^2$, where $u_i, u_j \in \mathbb{R}^d$ are the embedded coordinates.

Assume that CCA is applied to $\X_1$ and $\X_2$, which yields two  $n \times d$ matrices, $\widetilde{\X_1}$ and $\widetilde{\X_2}$,   the embedded configuration matrices. 

For the same multiview data, $\X_1$ and $\X_2$,
let \[ Z= \left[
\begin{array}{cc}
                  \X_1 & \bm{0} \\
                \bm{0}   & \X_2 
                \end{array}
                \right].
                \]
Let $W=\left[
                \begin{array}{cc}
                \bm{0} & I_n \\
                I_n   & \bm{0}
                \end{array}
                \right]$ be a $2n \times 2n$ weight matrix. We can assume that $W$ is an adjacency matrix that represents a graph that is bipartite, and the only edges lie between the $i^{th}$ and $(n+i)^{th}$ vertices (which correspond to matched pairs in earlier chapters) for $i \in \{1,\ldots,n\}$. The degree matrix for this graph is then $D=I_{2n}$. The graph laplacian is $L=D-W$. Assume the constraint that the embedding coordinates of the $i^{th}$ point  $\widetilde{Z_i}=p^{T}Z_i$ are introduced for some $p \in \mathbb{R}^d$, i.e., $p$ is a projection vector. We call this constraint the linearity condition. Then, the embedding of the $i^{th}$ point of the $2n$ points via Spectral Embedding for the weighted adjacency matrix $L$ is $\widetilde{Z_i}$, where $\widetilde{Z_i}= \left[ \widetilde{X_{1i}} \onespace \, \bm{0} \right]$ or  $\widetilde{Z_i}=\left[ \bm{0} \onespace \, \widetilde{X_{2i}} \right]$ and $\widetilde{X_{1i}}$ and $\widetilde{X_{2i}}$ are  the $i^{th}$ rows of $\widetilde{\X_1}$ and $\widetilde{\X_2}$ yielded by CCA.  As the authors note in \cite{CCAviaSpectralEmbed}, from $W$, we can take intra-view similarities into account (which means preserving more fidelity) and choose the diagonal block matrices in $W$ to be nonzero.
                %(e.g. \exp(-d(\X_1i,\X_1j) for $(i,j)^{th} entry in the first diagonal block of  $W$. 
                This would be akin to a JOFC-type embedding because the commensurability criterion is accounted for by using an identity matrix as the off-diagonal block matrix in $W$, and the fidelity criterion is accounted for by the nonzero diagonal block matrices in $W$.
                
             As mentioned in \ref{MDS_SpectralEmbed}, the joint embedding of a dissimilarity matrix  via cMDS  is equivalent to spectral embedding under certain conditions.
Consider to the spectral embedding generalization of CCA we have just presented, using the multiview data $Z$ and weight matrix $W$ obeying the linearity condition.  There is an equivalent classical MDS embedding with an omnibus dissimilarity matrix $M$ for which $\tau(M)=-\frac{1}{2}JMJ^T$  corresponds  to the  pseudo-inverse of  $L=D-W$.



\section{Generalized CCA: $K>2$ \label{sec:GenCCA}}

Whereas CCA is defined for $K=2$ conditions, multiple generalizations are available because the correlation between more than two configurations can be defined in multiple ways \cite{generalCCA}. Let $X_1,\ldots,X_K$ be random vectors for which a generalized CCA representation will be computed.  Consider the first set of canonical variates to be computed, $Z_1^{(1)},\ldots,Z_K^{(1)}$. Denote  the correlation matrix of  $Z_1^{(1)},\ldots,Z_K^{(1)}$ by $\Phi^{(1)}$.   The following three criteria  are proposed in \cite{generalCCA}:
\begin{itemize}
\item SUMCOR. Maximize the sum of the elements of $\Phi^{(1)}$ : ${\1}^T(\Phi^{(1)}){\1}$,
\item MAXVAR. Maximize the largest eigenvalue of $\Phi^{(1)}$ : $\lambda^{(1)}_1$ ,
\item  MINVAR. Minimize  the smallest eigenvalue of $\Phi^{(1)}$ : $\lambda^{(m)}_1$.
\end{itemize}
One can interpret all of these criteria as different norms on the correlation matrix.
%For $H_{A1}$ MINVAR might be a good choice because the exploitation task is to find evidence against the null and as favorable to the alternative as possible. 
%MINVAR should be small whenever there exists \emph{at least} one unmatched measurement under the alternative hypothesis. MAXVAR, in this case, wouldn't be necessarily small under alternative with respect to null.  MINVAR statistic should be stochastically smaller compared to null distribution. For $H_{A2}$ MAXVAR is appropriate, since under the alternative hypothesis, MAXVAR should be very small compared to under the null hypothesis. 
%For $H_{A2}$, MAXVAR is appropriate because under the alternative hypothesis, MAXVAR should be very small compared its value to under the null hypothesis. It is quite possible that other generalizations are appropriate for our task.
An interesting question that will not be addressed here is whether one of these generalizations is more appropriate for $H_{A1}$ or $H_{A2}$.
We chose to use  $ H_{A1}$ as the  alternative hypothesis and the SUMCOR criterion  as the generalization of CCA.
