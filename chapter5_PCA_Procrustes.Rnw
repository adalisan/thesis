
\chapter{Procrustes Analysis for Data Fusion}
\label{chap:PoM}
\chaptermark{Procrustes Analysis}



\section{Procrustes Analysis}
Given two configurations of $n$ points in $d$-dimensional Euclidean space, Procrustean methods fit one configuration to the other so that the points align as well as possible in the $\ell_2$-sense. Let us denote the configurations by two $n \times d$ matrices: ${\X}_1$, ${\X}_2$. The most general version of this  method  seeks an affine transformation with $\rho$ \emph{only} rotation, reflection, scaling and translation components that transforms the points in the configuration ${\X}_2$  to  align with the target configuration ${\X}_1$. The transformation $\rho$  is chosen such that the sum of squares of the distances from each $\rho$-transformed point of ${\X}_2$  to its corresponding point is minimized. For notational convenience, let $\varrho_{\rho}:\Mat_{n \times d} \rightarrow \Mat_{n \times d}$ be the mapping applied to a configuration matrix such as ${\X}_1$ whose rows correspond to the point coordinates, when each point is mapped by $\rho$. That is, $\varrho_{\rho}$ applies the $\rho$ transformation to every point in the configuration. For example, if $\rho$ is the identity map,    $\varrho_{\rho}(\X_1)=\X_1$.

It is also possible to introduce extra constraints on the affine transformation, such as requiring the translation component to be a zero vector (if both of the point configurations are zero-centered) or setting the scaling component to 1 (if only rigid transformations are allowed). First, let us consider the general case where $\rho(z)=sz\mathbf{Q}+\mathbf{t}$,  where $\mathbf{Q} \in \Mat_{d\times d}$, $s \in (0,\infty)$, $\mathbf{t}\in \Real^{d}$. For configuration matrices, the mapping is  $\varrho_{\rho}({{\X}_2}) = s{\X}_2\mathbf{Q}+\1\mathbf{t}^T$. We will derive  the components of the Procrustean transformation, $s$, $\mathbf{Q}$ and $\mathbf{t}$, following \cite{borg+groenen:1997}.

We seek to  minimize 
\begin{align*}
\mcL(s,\mathbf{Q},\mathbf{t}) &= \|{\X}_1 - (s{\X}_2\mathbf{Q}+\1 \mathbf{t}^T)\|_F^2  
\\ &= \Tr {\left({\X}_1 - (s{\X}_2\mathbf{Q}+\1 \mathbf{t}^T) \right)^T \left({\X}_1 - (s{\X}_2\mathbf{Q}+\1 \mathbf{t}^T) \right)}.
\end{align*}

Setting the gradient $\frac{\partial \mcL}{\partial \bm{t}}=2 \left({\X}_1^T \1 - (s\mathbf{Q}^T {\X}_2^T \1+ n\mathbf{t})\right)$ to $\mathbf{0}$ ,
we solve for $\hat{\mathbf{t}}$: $$\hat{\mathbf{t}}= n^{-1} \left({\X}_1^T \1  -s\mathbf{Q}^T {\X}_2^T \1 \right).$$ 

Putting $\hat{\mathbf{t}}$ into  $\mcL(s,\mathbf{Q},{\mathbf{t}})$, we obtain 
\[\mcL(s,\mathbf{Q},\hat{\mathbf{t}}) = \Tr \left((\bm{I}_n- \frac{\1 {\1} ^T}{n}){\X}_1 - (s{\X}_2\mathbf{Q} )\right)^T \left((\bm{I}_n- \frac{\1 {\1} ^T}{n}){\X}_1 - (s{\X}_2\mathbf{Q})\right).\]
Let us denote the centering matrix $\left(\bm{I}_n- \frac{\1 {\1} ^T}{n}\right)$  with $\bm{H}$. 
Setting  $\frac{\partial \mcL}{\partial s}=2 \Tr {s{{\X}_2}^T  \bm{H} {{\X}_2}}-2 \Tr {  {{\X}_1}^T \bm{H} {\X}_2\mathbf{Q}}=\bm{0}$,
we obtain $\hat{s}=\frac{ \Tr {  {{\X}_1}^T \bm{H} {\X}_2\mathbf{Q}}}{\Tr {{{\X}_2}^T  \bm{H} {{\X}_2}}}$.
Putting $\hat{s}$ into $\mcL(s,\mathbf{Q},\hat{\mathbf{t}})$, we obtain
$\mcL(\hat{s},\mathbf{Q},\hat{\mathbf{t}}) = 
\Tr{{\X}_1^T \bm{H} {\X}_1}- 
\frac{ \left({ \Tr{  {\X}_1^T \bm{H} {\X}_2\mathbf{Q} }}\right)^2}
{\Tr{{\X}_2^T \bm{H} {\X}_2}} $.

The final step is computing  $\mathbf{Q}$. 
Note that the only term in $\mcL(\hat{s},\mathbf{Q},\hat{\mathbf{t}})$  that depends on $\mathbf{Q}$ is  $\left[\Tr \left( {\X}_1^T \bm{H} {\X}_2\mathbf{Q}\right)\right]^2$. 
Subject to $\mathbf{Q}\mathbf{Q}^T=\mathbf{Q}^T\mathbf{Q}=\bm{I}_n$, minimizing   $-  \left[\Tr \left( {\X}_1^T \bm{H} {\X}_2\mathbf{Q}\right)\right]^2$ is equivalent to minimizing  $-\Tr \left( {\X}_1^T \bm{H} {\X}_2\mathbf{Q}\right)$ (Because $\hat{s}>0$, minimizing $-x$ is the same as minimizing $-x^2$ given a constraint on $x$). Thus,  \begin{equation}
\hat{\mathbf{Q}}=\argmin_{Q^TQ = \bm{I}_n}{-\Tr \left( {\X}_1^T \bm{H} {\X}_2\mathbf{Q}\right)}. \label{q_procrustes}
\end{equation} Therefore, the solution for $\mathbf{Q}$ in the general Procrustes problem is equivalent to the solution of the orthogonal Procrustes problem.

For the orthogonal Procrustes problem, we seek  an orthonormal matrix $\mathbf{Q}^*$ that minimizes the sum of squared distances between the  target configuration ${\X}_1$ and  the configuration ${\X}_2$ transformed by $\mathbf{Q}^*$, i.e., 
 $\mathbf{Q}^* = \argmin_{\mathbf{Q}^T\mathbf{Q} = \bm{I}_n} \|{\X}_1 - {\X}_2\mathbf{Q}\|_F^2   $,
 where $\|\cdot\|_F$ is the Frobenius norm on matrices.
Simplifying the norm expression, we obtain $\|{\X}_1 - {\X}_2\mathbf{Q}\|_F^2 = \Tr {({\X}_1 - {\X}_2\mathbf{Q})^T({\X}_1 - {\X}_2\mathbf{Q})}= \Tr {({\X}_1^T{\X}_1 + {\X}_2^T{\X}_2)} - 2\Tr{ ({\X}_1^T {\X}_2\mathbf{Q})}$. Because the first term is independent of $\mathbf{Q}$, we can ignore that term. The second term  is equivalent to \eqref{q_procrustes} when  ${\X}_1^T\1 =  {\X}_2^T\1=\bm{0}$. Then, the solution for $\mathbf{Q}^*$ is the  $d\times d$ orthogonal matrix that maximizes $\Tr{ ({\X}_1^T {\X}_2\mathbf{Q})}$.

 Consider the singular value decomposition $\mathbf{{\X}_1^T {\X}_2}=\bm{U} \bm{\Sigma} \bm{V}^T$. The expression to be minimized can be written as $\Tr {\bm{U} \bm{\Sigma} \bm{V}^T \mathbf{Q}}$, which is equal to  $\Tr { \bm{V}^T \mathbf{Q} \bm{U} \bm{\Sigma}} $ due to the circular invariancy of the trace operation.
 
 Note that for an orthogonal matrix $T$ and a diagonal matrix $\Lambda$ with non-negative entries ($\Lambda_{ii}\geq 0$),
 \[
 \Tr{ T \Lambda } \leq \Tr{ \Lambda} \] with equality if $T=\bm{I}$.
 
 Note that $\bm{\Sigma}$ is diagonal with nonnegative entries and that $\bm{Z}=\bm{V}^T \mathbf{Q} \bm{U}$ is also orthogonal. To see why $\bm{Z}$ is orthogonal, consider 
 \begin{align*}
 \bm{Z}\bm{Z}^T &= \bm{V}^T \mathbf{Q} \bm{U} \bm{U}^T \mathbf{Q}^T  \bm{V} \\
 &= \bm{V}^T \mathbf{Q} \bm{I}_n \mathbf{Q}^T  \bm{V}\\
 &= \bm{V}^T \bm{I}_n  \bm{V} \\
 &= \bm{I}_n
 \end{align*} 
 Each step is justified by the fact that the SVD of $\mathbf{{\X}_1^T {\X}_2}$ results in matrices  $U$ and $V$ with orthogonal columns, and $R$ is already known to be orthogonal.
 Therefore,  $$\Tr { \bm{V}^T \mathbf{Q} \bm{U} \bm{\Sigma}}  \leq \Tr \bm{\Sigma}$$ 
with equality if  $\bm{V}^T \mathbf{Q} \bm{U} = \bm{I}_n$. The solution  that achieves the bound is $\hat{\mathbf{Q}}= \bm{V}  \bm{U}^T$.

\section{Procrustes Analysis for Manifold Matching}

Because separate  condition dissimilarities are available, a straightforward approach is to embed each conditional dissimilarity matrix, $\Delta_1$ and $\Delta_2$, separately  in  $d$-dimensional Euclidean space (we denote these embedded configurations by the configuration matrices ${\X}_1$ and ${\X}_2$, respectively) and then find a mapping function ${\rho}^* : \mathbb{R}^{d}\rightarrow\mathbb{R}^{d}$, that maps each point in ${\X}_2$  approximately to its corresponding point in ${\X}_1$. This approach can be considered a specific example of the general setting in  ~\autoref{fig:Fig1} in which the commensurate space is $d$-dimensional  Euclidean space, $\rho_1$  is the identity map, and $\rho_2={\rho}^*$. 

The mapping ${\rho}^*$ is estimated by using Procrustes Analysis on the training data. This estimate, $\rho$,   makes the separate MDS embeddings as commensurate as possible. Once such a mapping is computed, one can OOS embed new dissimilarities for each condition (separately) and use $\rho$ to make the embeddings commensurate.
One can then compute the test statistic $\tau$ (the distance between commensurate embeddings) for  the hypothesis testing problem in \autoref{chap:match_detection}. This approach will be referred to  as  P$\circ$M -- Procrustes$\circ$MDS.

Note that the Procrustes transformation $\rho$  is limited to  an affine transformation consisting of rotation and reflection and possibly also scaling components. The optimal mapping might very well be nonlinear. If a larger class of mappings is considered, this would result in a smaller model bias but also in a larger variability for the mapping function. By only considering the class of linear transformations, it is possible to learn $\rho$ with the limited sample size.

\subsection{Relation of P$\circ$M and JOFC} 

In this section, we explain where Procrustes$\circ$MDS stands in relation to the Fidelity-\-Commensurability tradeoff view of multiview dissimilarities.

Suppose, in equation \eqref{eq:FidCommSep}, that the weights are chosen to be $w_{ijk_1k_2}=w$ for commensurability terms and $w_{ijk_1k_2}=1-w$ for fidelity terms. For the resulting weight matrix $W$, define 
\begin{equation}
f_w(\mathcal{D}(\cdot),M) = \sigma_W(\cdot) \label{fid-comm-tradeoff-func}
\end{equation}
 where $M$ is the omnibus matrix obtained from  a given pair of dissimilarity matrices, $\Delta_1$ and $\Delta_2$, as in equation \eqref{omnibus}.   As $w$ goes to 0, the configuration embedded by JOFC converges to a configuration equivalent to (up to rotation and reflection)  the configuration embedded by P$\circ$M.


\begin{thm}
Define $\sigma(\cdot)=\sigma_{W=\bm{1}}(\cdot)$ (unweighted raw stress), where $\bm{1}$ is a matrix of 1's.
 Let $\mathbf{X}_1$ and $\mathbf{X}_2$ be the corresponding $n\times p$ configuration matrices with column means of $\bm{0}$ (obtained from separately embedding  $\Delta_1$ and $\Delta_2$ by minimizing the raw stress $\sigma(\cdot)$ ). 
Let  $\mathbf{Q}=\argmin_{\mathbf{P^T}\mathbf{P}=\mathbf{P}\mathbf{P^T}=\mathbf{I}}\|{\mathbf{X}_1-\mathbf{X}_2}\mathbf{P}\|_{F}^2$ ,   $\mathbf{\tilde{X}}_2= \mathbf{X}_2\mathbf{Q}$, 
and let  
$\mathbf{X}=\left[\begin{array}{c}
\mathbf{X}_1\\
\mathbf{\tilde{X}}_2
\end{array}\right]$.

For $w>0$, let $\mathbf{Y}_{w} = \left[\begin{array}{c}
\mathbf{Y}_1\\
\mathbf{Y}_2
\end{array}\right]$  be  a $2n \times p$ configuration matrix obtained by the minimization of 
$ f(\mcY, M) =(1-w)\left({\sigma{(\mcY_1)}}+{\sigma{(\mcY_2)}}\right)+w\|{\mcY_1-\mcY_2}\|_{F}^2 $ with respect to  $\mcY=\left[\begin{array}{c}
\mcY_1\\
\mcY_2
\end{array}\right]$ with the constraint that $\mcY_1$ and $\mcY_2$ are two $n \times p$ configuration matrices with column means $\bm{0}$. Then, $$lim_{w\rightarrow0}\mathbf{Y}_{w}=\mathbf{X}\mathbf{R}$$ for a $p\times p$ orthogonal matrix $\mathbf{R}$. ($\mathbf{R}$ is a transformation matrix with a rotation and possibly a reflection component.)
\end{thm}
 
\section{Generalized Procrustes Analysis ($K>2$) \label{sec:GenProcrustes}}
The Generalized Procrustes analysis is the extension of Procrustes analysis to more than two configurations of points. This extension has been studied in \cite{GPCA}. Suppose we have $K$ configurations: $\X_1,\X_2,\ldots \X_K$. We wish to find $K$ Procrustean transformations  $\tau_k(\X_k)= s_k\X_k\bm{Q}_k+\bm{t}_{k}$ such that \[\sum_{kl}\|\tau_k(\X_k)-\tau_k(\X_k)\|^2_F\] is minimized. This problem does not have a single-step analytical solution for all of the components, similar to the original Procrustes analysis problem. The translation components, $\bm{t}_{k}$, of the transformations can be solved by subtracting the column sums of $\X_k$ ($\1\X_k$). The rotation/reflection components, $\bm{Q}_k$, can be solved iteratively by minimizing the error function with respect to $\X_k$ and keeping all other $\X_l, l \neq k$ constant for each $k$ in turn. After the convergence of iterative solutions for $\bm{Q}_k$, the scaling components, $s_k$, can be solved for analytically.

Using Generalized Procrustes Analysis (GPA), we can obtain estimates for $K$ mapping functions $\rho_k$ depicted in \ref{fig:multisensor} when $K>2$. Given $K$  dissimilarity matrices $\Delta_k,k=1,\ldots,K$, we would compute separate MDS embeddings of  $\{\Delta_k \}$  followed by GPA of all the embeddings. The separate embeddings mapped via $\{\tau_k \}$  would give us a single commensurate representation in which the disparate dissimilarities can be compared. New dissimilarities $\{ \bm{D}_k \}$ can be OOS embedded  and mapped by the same Procrustean transformations  $\{\tau_k \}$ to the commensurate space. We will use this approach  for the match testing test presented in \autoref{chap:match_detection}, when $K>2$. Simulation results are presented in \ref{k_more_than_two_experiment}.


