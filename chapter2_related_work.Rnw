\chapter{Related Work}
\label{chap:RelatedWork}
\chaptermark{Related Work}



\section{Multiple View Learning}
\label{sec:MultiViewLearn}
When data are collected using a multitude of sensors or under significantly different environmental conditions, we refer to  the data setting as a multiple view setting, in which each ``view'' provides possibly complementary  information about the observed objects\footnote{We use the term ``object''  loosely because  the observed objects could be topics or concepts and the collected data could be text documents about those topics or images that are related to a concept, for example.}. Multiple view learning seeks to exploit these views simultaneously to be more successful in the learning task.


In data settings for multiview learning, the data are observations from $K\geq 2$ views,
 where both the relationship between the features from different views and the relationship between the features and the quantity to be predicted are unknown. The objective is to train the best predictor. It is possible to use all of the features in different views (i.e., concatenate the observation vector from each view)  and perform feature selection without considering from which view a feature is obtained. However, this ignores the fact that the modalities can be quite diverse and that combining features from different modalities is not always meaningful. Consider features extracted from an image and  an audio segment as features from different modalities. A classifier that treats these features in the same way without considering their modalities is  unlikely to perform well.
It is more reasonable to use the prior information that the features in the same modality are much more likely to be correlated or commensurate with each other than features in different views and use predictors more suited to each modality if the different modalities are diverse.

 Multiple View Learning is a burgeoning field, and there are many cases where one has to leverage many different related datasets for an inference task. For example, for learning  tasks related to webpages (such as webpage categorization and ranking of relevant webpages), both the content of the webpages and the hyperlink structure between the webpages can be used. 

For social networks, people have different relationships with other people in their networks; networks  may be based on similarity of interests, geographical proximity and job relationships, among factors. Combining information from different social networks would provide a more complete perspective of the underlying social life of the people in the network, and one would expect a better performance for all kinds of  inference based on the complete social network data compared with a social network based on a single type of relationship (assuming one does not fall into the trap of overfitting due to having more features in the complete social network data).

In addition, when it is necessary to collect more data, it is often easier to collect data in different modalities than it is to collect more samples in a single modality. For example, in medical studies, it is much easier to  collect medical data from already recruited patients  compared with recruiting new patients. Data from different modalities might provide complementary information and could result in much more effective predictors, as opposed to data from a single modality that provides diminishing returns with increasing sample size.

Some of the well-studied subfields of machine learning, such as dimensionality reduction, are also relevant to our multiple view setting. 
As more data are collected, a low-dimensional representation of the data is necessary to be learned to avoid the curse of dimensionality. An interesting question is how dimensionality reduction can be performed in a multiple view setting: is it better to perform dimensionality reduction separately for each modality and concatenate the resulting low-dimensional representations or to find a joint low-dimensional representation for all of the modalities simultaneously? This is a question that we attempt to answer for the data settings we discuss in this thesis.

In the case of missing data, observations of features in the same view could  be missing altogether. In the case of such structurally missing data, it makes sense to train an ensemble of predictors that use features from different views independently, so that  accurate predictions can be made even if observations from some of the views are missing.


In  \cite{Amini2009}, the authors  discuss  an example of multiview learning problems: classification of a multi-lingual document corpus. They  co-train  classifiers for single-language data that jointly minimize  the loss in each single language along with the disagreement between classifiers on training examples. Their findings support the intuition that  classifiers based on multiview learning perform   better than  classifiers trained with  data from only a single view.

In \cite{Sun2005}, the inference task is classification. Features from multiple modalities are fused via canonical correlational analysis, a classical statistical method  which computes maximally correlated projections of data. This fusion leads to better classification performance compared with the original set of features in a typical classification problem.

A popular approach to multiview learning is multiple kernel learning, which is the task of learning a  kernel matrix for each modality and combining these kernels in an optimal way (with respect to the inference task). For $K$ views, let the $i^{th}$ datum for the $k^{th}$ view be represented as $X_{ik},\quad i \in \{1,\ldots n\}, k \in \{ 1,\ldots, K \}$. For the data in the same $k^{th}$ view, let ${\mcK}_{k}$  be kernel matrix defined for that view, whose $(i,j)^{th}$ entry is $ \mck_{k} (X_{ik},X_{jk}) $ where $i,j \in {1, \ldots, n}$.  Because any convex combination of the kernels, \footnote{ A convex combination of the kernels is $\sum_{k'=1}^{K}{\alpha_{k'} \mck_{k'} }$ such that $\sum_{k'=1}^{K}{\alpha_{k'}}= 1$ and  $\alpha_{k'} \geq 0 ,k' \in \{ 1,\ldots, K \} $.} is also a kernel, it is possible to compute a joint kernel $\mck$ that uses all of the multiview data by a convex  combination of the kernels in each view. Assuming that a kernel can be defined for each view, the learning problem is  to find the optimal (for the inference task) set of coefficients $\{\alpha_k\}$. These parameters are usually estimated using training data. Denoting the optimal  $\{\alpha_k\}$ by  $\{\hat{\alpha}_k\}$,  $\hat{\mcK} = \sum_k{\hat{\alpha}_k \mcK_{k} } $  is the optimal kernel whose  $(i,j)^{th}$ entry is  $ \sum_k{\hat{\alpha}_k {\mck_k} (X_{ik},X_{jk})} $.  Given a new datum $x=[x_1 \ldots x_{k}]$ which consist of $K$ views, the kernel function for each view, $\mck_{k}$, along with $\{\hat{\alpha}_k\}$ is used to compute the inner product for the joint kernel: 
\[
{\mck}(x,.)= \sum_{i}\sum_{k}{\hat{\alpha}_k {\mck_k} (x_{k},X_{ik})} .
\]

There are many papers on ``Multiple Kernel Learning''  in the literature~\cite{McFee:2011:LMS:1953048.1953063,Lin2009,Lanckriet2004}, which are reviewed in a comprehensive survey \cite{MKLSurvey}.
Choi et al. \cite{Choi:2008:MIM:1619995.1620064} use the Markov random walk interpretation of multiple kernel matrices to find a single kernel matrix that depends on  the joint probability of the random walks in different views. 
\cite{ZhouBurges2007a} is another work that uses the random walk interpretation to deal with multiview data. The learning task in  \cite{ZhouBurges2007a} is spectral clustering with multiple graphs.

\section[Transfer Learning and Domain Adaptation]{Transfer Learning \label{sec:translearn}}

Methods that utilize training data in one domain as auxiliary information for learning in another domain are categorized as ``transfer learning'' \cite{TransLearnSurvey}. Sometimes, the source domain and the target domain are actually the same, but the distribution of the data is different,  due to the inherent differences between the way in which the training and test data were collected. 
We call this phenomenon sample selection bias or covariance shift (SSB/CS) \cite{Zadrozny2004a,TransLearnSurvey}.

According to \cite{Hand2006a}, this SSB/CS phenomenon is commonly seen in real-life data analysis problems and is usually understated by practitioners. To evaluate novel classifiers, the classifiers are trained on a portion of the available data and tested on the held-out data. Therefore, in the evaluations of classifiers, the assumption that training and test data come from the same distribution is usually valid. However, any performance improvements that a new classifier model has over the baseline would be overwhelmed by the sample selection bias. Thus, one should be skeptical about improving accuracy scores for benchmark datasets in machine learning and treating them as evidence of progress.

We now clarify the differences between transfer learning and SSB/CS problems.  Let $y$ denote the random variable for the class label  for classification or the dependent variable for regression and $X$ denote the random variates that we use for the learning task. We use the common assumption that the data are $\iid$.  
Suppose we have two domains  ${\mcD}_s$ and ${\mcD}_t$ from which the training data and test data, respectively, are collected. These are called the source and target domains, respectively.
The training data $\left(X_i,y_i\right) \in {\mcD}_s$ and are drawn from the joint distribution $\Pry(X,y)$. 
The test data $\left({X'}_i,{y'}_i\right) \in {\mcD}_t$ and  are drawn from the joint distribution $\Pry'(X,y)$.
The most common objective is to infer $\Pry'(X,y)$  given an  $\iid$ sample of $\left(X_i,y_i\right) \in {\mcD}_s$. 
The learning task is usually to minimize $E[\ell(y, \argmax_y{\hat{\Pry}'(y | X) } )]$,  with respect to  $\hat{\Pry}'(y | X)$
where $\ell(\cdot,\cdot)$ is the loss function chosen for the task, $\hat{\Pry}'(y | X)$  is an approximation  to $\Pry'(y|X)$ based on the training and the test data. Basically, we require an inference method for the data distribution of the target domain   $\hat{\Pry}'(y | X)$ that minimizes the expected loss for prediction in the target domain.

In the classical supervised learning setting, the source and target domains are the  same, and  $\Pry(X,y)$ is assumed to be the same as $\Pry'(X,y)$. In the \emph{covariate shift} problem setting, the target domain is the same as the source domain, ${\mcD}_s={\mcD}_t=\mcD$, and $\Pry(y|X) \approx \Pry'(y | X)$, whereas $\Pry(X) \neq\Pry'(X)$.  When we cannot make either of the assumptions $\Pry(X) = \Pry'(X)$ or $\Pry(y|X) = \Pry'(y | X)$, we have the \emph{sample selection bias} problem \cite{Zadrozny2004a}.


 In some learning problems, the source and target domains are different ${\mcD}_s \neq {\mcD}_t$, and all or a considerable portion of  the labels $\{{y'}_i\}$  in $\left({X'}_i,{y'}_i\right) \in {\mcD}_t$ are missing. In this case, domain adaptation methods allow for the exploitation of both the  data in the source domain $\{\left({X}_i,{y}_i\right)\}$ and the  data in the target domain  $\{\left({X'}_i,{y'}_i\right)\}$ (where some ${y'}_i$ might be missing) to construct a good predictor for the target domain\cite{DaumeIII2006,Ben-David_Dom_Adapt2007,Ling2008,Pan2008a}.

Various ``domain adaptation'' approaches\cite{Pan2008a,LowRankSharedConceptChen2012a} assume the existence of mappings to a common latent space ${\mcD}_{com}$,  $\Psi_s:{\mcD}_s \rightarrow {\mcD}_{com} $ and $\Psi_t:{\mcD}_t \rightarrow {\mcD}_{com}$  such that the class conditional distributions $\Pry(\Psi_s(X) |y) \approx \Pry(\Psi_t(X') | y'=y)$.  If these mappings to the commensurate space can be inferred, then they can be used to predict $y'$ given $\Psi_t(X')$, even if no $\left({X'}_i,{y'}_i\right) \in {\mcD}_t$ pairs exist. 
%Other approaches infer a mapping from from the source domain to the target domain.
In \cite{Pan2008a}, for example, the distance between the conditional distributions $\Pry(\Psi_s(X) |y) $ $\Pry(\Psi_t(X') | y'=y)$ is computed using the  Maximum Mean Discrepancy
measure, and  the mappings $\Psi_s$ and  $\Psi_t$  are inferred using the minimization of the MMD measure.

\section[Manifold Matching]{Manifold Alignment}
Many efforts have been made toward solving ``manifold alignment'', which is a problem related to both our data fusion problem and the transfer learning problem (\autoref{sec:translearn}). ``Manifold alignment'' seeks to find correspondences between observations from different ``conditions''. The setting that is most similar to ours is the semi-supervised setting, in which a set of correspondences are given and the task is to find correspondences between a new set of points in each condition. In contrast, our hypothesis testing task is to determine whether any given pair of points is ``matched''. The proposed solutions follow a common approach in that they look for a common commensurate space such that the representations (possibly projections or embeddings) of the observations in the commensurate space match.

Note the similarity of the  description of ``manifold alignment''   to the latent space approach for domain adaptation. For both domain adaptation and manifold alignment, the objective is to find mappings to a  common space  so that the data in  one domain can be used for inference in the other domain.

Wang and Mahedavan~\cite{Wang2008} suggest an  approach that uses embedding followed by Procrustes Analysis to find a map to a commensurate space. Given a paired set of points, Procrustes Analysis~\cite{Sibson} finds a transformation from one set of points to another in the same space that minimizes the sum of squared distances, subject to some constraints on the transformation (see \autoref{chap:PoM}). In the problem mentioned in \cite{Wang2008}, the paired set of points correspond to low-dimensional embeddings of kernel matrices.   For the embedding step, Laplacian Eigenmaps were used, though their algorithm allows for any appropriate embedding method.

Zhai et al.~\cite{Zhai2010} find two projection matrices to minimize three terms in an energy function similar to our Joint Optimization of Fidelity and Commensurability (JOFC) approach (see \autoref{chap:FidComm}). One of the terms is the \emph{correspondence-preserving term}, which is the sum of the squared distances between corresponding points and is analogous to our commensurability error term. The other two terms are \emph{manifold regularization terms} and consist of the reconstruction error for a Locally Linear Embedding of the projected points. These terms, which are analogous to fidelity terms, ensure that the projections in the lower dimension retain the structure of the original points by preserving the local neighborhood of points. For fidelity error terms in our setting, the preservation of the structure is accomplished by preserving the dissimilarities. 
Ham and Lee\cite{HamLee2005a} solve the problem in a semi-supervised setting using a similar approach: minimizing a cost function of three terms, with two terms for fidelity of embedding and one term for commensurability.

In a paper by Baumgartner et al.\cite{mri_joint_laplacian_embed},  the joint embedding of kernel matrices is formulated  as the optimization of  a single objective function that combines Fidelity and Commensurability terms. They use Local Linear Embedding Method for the joint embedding  and introduce  a tradeoff parameter between \emph{inter-dataset} and \emph{intra-dataset error} (corresponding to commensurability and fidelity, respectively) into the objective function. This approach could be used as another tool for the investigation of the tradeoff between Fidelity and Commensurability .




Three-way Multidimensional Scaling\cite{3wayNMDS,borg+groenen:1997} assumes that the  different ``conditions'' of the data are linear transformations of a single configuration and aims to find this single configuration and the linear transformation. In this approach, the mappings  $\{\rho_k \}$ that we define in \autoref{fig:gen-model} and \autoref{sec:data} are  assumed to be embeddings followed by linear transformations (see also \autoref{subsec:3wayMDS}).

%For our setting, one would first embed the in-sample dissimilarities via the three-way MDS, which would give the linear transformations that map from group configuration to individual configurations under each condition. This is followed by out-of-embedding the OOS dissimilarities and using the inverse of the transformation matrices to find the OOS embeddings with respect to the group configuration. Because the OOS embeddings are commensurate, the test statistic can be computed as the distance between the OOS embeddings. 


