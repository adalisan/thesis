\chapter{Related Work}
\label{chap:RelatedWork}
\chaptermark{Related Work}



\section{Multiple View Learning}
\label{sec:MultiViewLearn}
When data are collected using a multitude of sensors or under significantly different environmental conditions, we refer to  the data setting as a multiple view setting where each ``view'' provide possibly complementary  information about the observed objects\footnote{We use the term ``object''  loosely, as the observed objects could be topics or concepts and the  collected data could be text, images, etc.}. Multiple view learning seeks to exploit these views simultaneously to be more successful in the learning task.

In data settings for multiview learning, the data consist of observations from $K>1$ views, where both the relationship between the features from different views and the relationship between the features and the quantity to be predicted are unknown. The objective is to train the best predictor. It is possible to use all the features in different views (\ie concatenate the observation vector from each view)  and carry out feature selection without regards to from which view a feature come from. However, this ignores the fact that the modalities can be quite diverse and combining features from different modalities is not always meaningful. Consider features extracted from an image  and an audio segment as features from different modalities. A classifier that treats these features in the same way without taking into consideration their modalities is  unlikely to have good performance.
It is more reasonable to use the prior information that the features in the same modality are much more likely to be correlated or commensurate with each other than features in different views and use predictors more suited to each modality, if the different modalities are diverse.

 Multiple View Learning is a burgeoning field, as there are many cases where one has to leverage many different related datasets for an inference task. For example, for learning  tasks related to webpages (such as webpage categorization, ranking of relevant webpages) both content of the webpages and the hyperlink structure between the webpages can be used. For social networks, people have different relationships with other people in their networks: networks  may be based on similarity of interests, geographical proximity, job relationships, etc. Combining information from different social networks would provide a more complete look into underlying social life of the people in the network and one would expect better performance in for all kinds of  inference based the complete social network data compared to a social network based on a single type of relationship (assuming one does not fall into the trap of overfitting, due to having more features in the complete social network data).

In addition, when it is necessary to collect more data, in a lot of cases it is easier to collect data in different modalities, than it is to collect more samples in a single modality. For example in medical studies, it is much easier to  collect medical data from already recruited patients, compared to recruiting new patients. Data from different modalities might provide complementary information and could result in much more effective predictors, as opposed to data from a single modality that provides diminishing returns with increasing sample size.

There are  also connections of this topic to well-studied machine learning subjects such as dimensionality reduction. As more data is collected, it is necessary to learn a low-dimensional representation of the data to avoid the curse of dimensionality. An interesting question is how to carry out dimensionality reduction in a multiple view setting: is it better to do dimensionality reduction separately for each modality, and concatenate the resulting low-dimensional representations, or find a joint low-dimensional representation  for all of the modalities simultaneously? This is a question we try to answer for data settings we discuss in this thesis.

In the case of missing data, observations of features in the same view could  be missing altogether. In the case of such structurally missing data, it makes sense to learn predictors that use features from different views independently, so that a predictor could make an accurate prediction even if observations from some of the views are missing.


In  \cite{Amini2009}, the authors  discuss  an example of multiview learning problems, classification of a multi-lingual document corpus. They  co-train  classifiers for single-language data that jointly minimize  loss in each single language along with disagreement between classifiers on training examples. Their findings support the intuition that  classifiers which are based on multiview learning perform   better than those classifiers who have been trained with only data from a single view.

In \cite{Sun2005}, the inference task is classification. Features from multiple modalities are fused via CCA to have better classification performance compared to the original set of features in a typical classification problem.

A popular approach to multiview learning is multiple kernel learning which is the task of learning a  kernel matrix for each modality and combining these kernels in an optimal way (with respect to the inference task). For $K$ views, let $i^{th}$ datum for $k^{th}$ be represented $X_{ik},\quad i \in {1,\ldots n}, k \in \{ 1,\ldots, K \}$ . For the data in the same view, let ${\mcK}_{k} = \left[ \mck_{k} (X_{ik},X_{jk}) \right] $ be the kernel matrix defined for that view.  Since any convex combination of the kernels $(\sum{\alpha_k \mck_{k} } , 0 \leq \alpha_k \leq 1)$ is also a kernel, it is possible to compute a joint kernel $\mck$ that use all of the multiview data by convex  combinations of the kernels in each view. Assuming that a kernel can be defined for each view, the learning problem is  finding the optimal (for the inference task) coefficients $\alpha_k$ for each view. These parameters are usually estimated using training data. Denoting the optimal  $\alpha_k$ by  $\hat{\alpha}_k$,  $\hat{\mcK} = \sum_k{\hat{\alpha}_k \mcK_{k} }  =  \left[ \sum_k{\hat{\alpha}_k {\mck_k} (X_{ik},X_{jk})} \right] $.  Given a new datum $x$, the kernel function for each view, $\mck_{k}$ ,  along with $\hat{\alpha}_k$ is used to compute the inner product for the joint kernel. 
\[
{\mck}(x,.)= \sum{\hat{\alpha}_k {\mck_k} (x_{k},X_{ik})} 
\]

Many papers on ``Multiple Kernel Learning"   exist in the literature~\cite{McFee:2011:LMS:1953048.1953063,Lin2009,Lanckriet2004} which are reviewed in a comprehensive survey \cite{MKLSurvey}.
Choi et al.\ \cite{Choi:2008:MIM:1619995.1620064} use the Markov random walk interpretation of multiple kernel matrices to combine into one kernel matrix. \cite{ZhouBurges2007a} is another work that uses the random walk interpretation to deal with multiview data. The learning task in  \cite{ZhouBurges2007a} is spectral clustering with multiple graphs.

\section[Transfer Learning and Domain Adaptation]{Transfer Learning \label{sec:translearn}}

Methods which utilize training data in one domain  as auxiliary information for  learning  in another domain fall under the term ``transfer learning'' \cite{TransLearnSurvey}. Sometimes the source domain and the target domain are actually the same, but the distribution of the data is different , e.g. inherent difference between training and test data due to collection conditions. We call this phenomenon sample selection bias or covariance shift (SSB/CS) \cite{Zadrozny2004a,TransLearnSurvey}.

According to \cite{Hand2006a}, this SSB/CS phenomenon is common in real life data analysis problems, and is usually understated by practitioners. For evaluation of newly designed classifiers, the classifiers are trained on a portion of the available data and tested on the held out data, so when evaluating classifiers, the assumption that training and test data come from the same distribution is usually valid. However any performance improvements that a new classifier model has over baseline would be swamped  by the sample selection bias. Thus, one should be skeptic about improving accuracy scores for benchmark datasets in machine learning and seeing them as evidence of progress.

Let us clarify the differences between transfer learning  and SSB/CS.  Let $y$ denote the random variable for the class label  for classification or the dependent variable for regression and $X$ denote the random variates we will use for the learning task. We will  make use of  the common assumption that data is $\iid$.  
Suppose we have two domains  ${\mcD}_s$ and ${\mcD}_t$ where the training data and test data is collected from, respectively. These are called the source and target domains, respectively.
The training data $\left(X_i,y_i\right) \in {\mcD}_s$ and are drawn from the joint distribution $\Pry(X,y)$. 
The test data $\left({X'}_i,{y'}_i\right) \in {\mcD}_t$ and have the distribution $\Pry'(X,y)$.
The most general objective is to infer $\Pry'(X,y)$  given $\iid$ sample of $\left(X_i,y_i\right) \in {\mcD}_s$. 
The learning task is usually minimizing  $\argmin E[\ell\{y, \argmax_y{\hat{\Pry}'(y \| X) } \}]$  
where $\hat{\Pry}'(y \| X)$  is an approximation  $\Pry'(X,y)$ based on the training and the test data. Basically we require an inference method for the data distribution of the target domain   $\hat{\Pry}'(y \| X)$ that minimizes expected loss for prediction in the target domain.

In the classical supervised learning setting, the source and target domains are the same and  $\Pry(X,y)$ is assumed to be the same as $\Pry'(X,y)$. If the target domain is the same as the source domain, ${\mcD}_s={\mcD}_t=\mcD$, but $\Pry(X) \neq\Pry'(X)$ while $\Pry(y\|X) \approx \Pry'(y \| X)$, this is the \emph{covariate shift} problem.  When we cannot make either of the assumptions $\Pry(X) = \Pry'(X)$ or $\Pry(y\|X) = \Pry'(y \| X)$, we have the \emph{sample selection bias} problem \cite{Zadrozny2004a}.


 In some learning problems, the source and target domains are different ${\mcD}_s \neq {\mcD}_t$ and all or a considerable portion of  the labels $\{{y'}_i\}$  in $\left({X'}_i,{y'}_i\right) \in {\mcD}_t$ are missing. In this case, domain adaptation methods allow the exploitation of both the  data in the source domain $\{\left({X}_i,{y}_i\right)\}$ and the data in the target domain  $\{\left({X'}_i,.\right)\}$ to construct a good predictor for the target domain\cite{DaumeIII2006,Ben-David_Dom_Adapt2007,Ling2008,Pan2008a}.
Various ``domain adaptation'' approaches\cite{Pan2008a,LowRankSharedConceptChen2012a} assume existence of mappings to a common latent space ${\mcD}_{com}$,  $\Psi_s:{\mcD}_s \rightarrow {\mcD}_{com} $ and $\Psi_t:{\mcD}_t \rightarrow {\mcD}_{com}$ to a  such that the class conditional distributions $\Pry(\Psi_s(X) \|y) \approx \Pry(\Psi_t(X') \| y'=y)$.  If these mappings to the commensurate space can be inferred, then they can be used to predict $y'$ given $\Psi_t(X')$ even if no $\left({X'}_i,{y'}_i\right) \in {\mcD}_t$ pairs exist. 
%Other approaches infer a mapping from from the source domain to the target domain.
In {Pan2008a}, for example, the distance between the conditional distributions $\Pry(\Psi_s(X) \|y) $ $\Pry(\Psi_t(X') \| y'=y)$ is computed via  Maximum Mean Discrepancy
measure and  the mappings $\Psi_s$ and  $\Psi_t$  are inferred by the minimization of MMD measure.

\section[Manifold Matching]{Manifold Alignment}
There have many efforts toward solving ``manifold alignment", which is a problem related to both our data fusion problem, and the transfer learning problem (\autoref{sec:translearn}). ``Manifold alignment" seeks to find correspondences between observations from different ``conditions". The setting that is most similar to ours is the semi-supervised setting, where a set of correspondences are given and the task is to find correspondences between a new set of points in each condition. In contrast, our hypothesis testing task is to determine whether any given pair of points is ``matched" or not. The proposed solutions follow a common approach in that they look for a common commensurate space such that the representations (possibly projections or embeddings) of the observations in the commensurate space match.

Note the similarity of the  description of ``manifold alignment''   to the latent space approach for domain adaptation. For both domain adaptation and manifold alignment, the objective is to find mappings to a  common space  so that the data in  one domain can be used for inference in the other domain.

Wang and Mahedavan~\cite{Wang2008} suggest an  approach that uses embedding followed by Procrustes Analysis to find a map to a commensurate space. Given a paired set of points, Procrustes Analysis~\cite{Sibson}, finds a transformation from one set of points to another in the same space that minimizes sum of squared distances, subject to some constraints on the transformation. In the case mentioned in \cite{Wang2008}, the paired set of points are corresponding low-dimensional embeddings of kernel matrices.   For the embedding step, they made the choice of using Laplacian Eigenmaps, though their algorithm allows for any appropriate embedding method.

 Zhai et al.~\cite{Zhai2010}  finds two projection matrices to minimize three terms in an energy function similar to our JOFC approach (see \autoref{chap:FidComm}). One of the terms is the \emph{correspondence preserving term} which is the sum of the squared distances between corresponding points and is analogous to our commensurability error term. The other two terms are \emph{manifold regularization terms} and consist of the reconstruction error for a Locally Linear Embedding of the projected points. These terms, analogous to fidelity, make sure the projections in the lower dimension retain the structure of the original points. For fidelity error terms in our setting, this is done by preserving dissimilarities. For manifold regularization terms, this is done by preserving the local neighborhood of points, such that close points are not mapped apart.
Ham and Lee\cite{HamLee2005a} solve the problem in semi-supervised setting by a similar approach, by minimizing a cost function of three terms, two terms for fidelity of embedding, one term of commensurability.
In another paper  the simultaneous embedding is written  as a single function  that combines Fidelity and Commensurability terms. By using Local Linear embedding,  they are able to formulate the embedding as a function of a $2n \times d$ configuration matrix, and a tradeoff parameter between \emph{inter-dataset} and \emph{intra-dataset error} (corresponding to commensurability and fidelity, respectively). This approach could be used as another tool for the investigation of JOFC.



Another approach is Three-way Multidimensional scaling\cite{3wayNMDS,borg+groenen:1997}.
 This approach assumes the  different ``conditions" of the data are linear transformations of a single configuration and aims to find this single configuration and the linear transformation. With this approach, the mappings  $\{\rho_k \}$ that we define in \autoref{fig:gen-model} and \autoref{sec:data} are  assumed to be embeddings followed by linear transformations. See also \autoref{subsec:3wayMDS}.
 %For our setting, one would first embed the in-sample dissimilarities via the three-way MDS, which would give as the linear transformations that map from group configuration to individual configurations under each condition. This is followed by out-of-embedding the OOS dissimilarities, and use the inverse of the transformation matrices to find the out-of-sample embeddings with respect to the group configuration. Since the out-of-sample embeddings are commensurate, the test statistic can be computed as the distance between the OOS embeddings. 
