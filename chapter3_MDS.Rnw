\chapter{Variants of Multidimensional Scaling and Principal Components Analysis}
\label{sec:MDS}
\chaptermark{Variants of Multidimensional Scaling and  Principal Components Analysis}



\section{Multidimensional Scaling}
Multidimensional Scaling (MDS)~\cite{CMDS,borg+groenen:1997,duin2005dissimilarity}  is the general term that is used to describe methods to    embed dissimilarities as points in a Euclidean space. 
The embeddings are a configuration of points in the Euclidean space  with a chosen dimension $d$  such that the distances between the embeddings are as close as possible (in various senses) to the respective original  dissimilarities. 
Different criterion functions can be used to measure how close the distances are to the given dissimilarities, thereby leading to different embedded configurations. These different variants of MDS can be described using a single formulation, which are introduced in \autoref{sec:MDSvariant}.

Consider a  set of n objects. Let us denote the set  $\{1,\ldots,n\}$ by  $\oneton$. For each pair  of objects with indices $i,j \in \oneton$, the dissimilarity value, denoted by $\delta_{ij}$, is a nonnegative real number that quantifies how dissimilar those two objects are. The collection of these values form the matrix $\Delta$, which is  an $nxn$ dissimilarity matrix. %While, in general, dissimilarities between finite set of objects are being considered; for a dissimilarity matrix, we can consider the objects to be points in a space, whose neighborhood relationships are defined by the dissimilarity measure.

The dissimilarities have to satisfy $\delta_{ij}\geq 0$,  $\delta_{ij}=0$ if and only if $i = j$, and $\delta_{ij}=\delta_{ji}, \quad \forall i,j \in \oneton$. Therefore,  $\Delta$ is nonnegative, hollow, symmetric and its only zero entries appear on the diagonal. If, in addition, each triplet of dissimilarities $\delta_{ij}$, $\delta_{ik}$ and $\delta_{ik}$, $i,j,k \in \oneton$  satisfies the triangle inequality, then $\Delta$ is called a distance matrix.


\section{Different criteria for MDS \label{sec:MDSvariant}}
Multidimensional Scaling methods find a configuration of points $\{ \bm{x_i}; i\in \oneton \}$ in a finite-dimen\-sional Euclidean space, whose interpoint distances approximate the given dissimilarities  $\{\delta_{ij};  i,j \in \oneton \}$. There are various variants of MDS that use different measures of error for this approximation. In general, the criteria minimize the discrepancy between $f(\delta_{ij})$ and  $d(\bm{x_i},\bm{x_j})$ $\forall i,j \in \oneton$, with respect to $\{\bm{x_i}\}$ where $d(\cdot,\cdot)$ is the Euclidean distance function and $f(\cdot)$ is a monotonically increasing function that depends on the MDS variant. Depending on whether the MDS variant is  ``metric'' or ``non-metric'', $f(\cdot)$ is either a linear or nonlinear transformation. Specific variants of MDS are defined by $f(\cdot)$ and the measure of discrepancy between  $f(\cdot)$ and $d(\cdot,\cdot)$. We call the latter \emph{the criterion function}, which is optimized with respect to the embedding coordinates.  We represent these coordinates in a $n times d$ configuration matrix $\X \in \Mat_{n times d}$, whose $i^{th}$ row is  $\bm{x_i}$. We also represent the $n \times n$ distance matrix whose entries are interpoint distances between the rows of $\X$ with the matrix-valued function, $\mathcal{D}(\X)$ and the $(i,j)^{th}$ entry of the distance matrix (the distance between $i^{th}$ and $j^{th}$  rows of $\X$) with $\mathcal{D}_{ij}(\X)$.


\subsection{Metric MDS}
 For metric MDS, transformations of the form  $f(z)=az+b$ are allowed where $a>0$ and $b$ are scalars.
 \subsubsection{Stress Criterion}
 Setting $f(z)=z$ and choosing  the discrepancy measure between the dissimilarities and distances of embedded points to be $\ell_2$, the  criterion of the resulting MDS variant is called the raw stress criterion. Additionally, weights ($\{ w_{st},\quad s,t \in \oneton \}$) can be introduced for each discrepancy term. Denoting the matrix composed of the weights by $W$, and the configuration matrix $\X$ that represent the embedded points, we write
 \begin{equation}
\sigma_{W}({\X})=\sum_{s,t \in \oneton} {w_{st}(\mathcal{D}_{st}({\X})-\delta_{st})^2  }\label{raw-stress}
\end{equation}
Subtypes of the Stress criterion are identified by different choices for $\{w_{st},{s,t \in \oneton}\}$ that depend on the original dissimilarities $\delta_{st}$.
For example, choosing all $w_{st}$ to be $\left[{\sum_{k,l \in \oneton}{\delta_{kl}^2}}\right]^{-1},$ $\forall s,t\in \oneton $ normalizes the stress so that the stress value is always between 0 and 1. One can compare different configurations by this standardized stress value and determine whether a configuration is a good fit based on this value.

%A slightly different weighting scheme is $w_{st}=\left[ \sum_{pq}{\delta_{pq}-\bar{\delta} } \, \right]^{-1}$, 
%where $\bar{\delta}=\frac{1}{(n)(n-1)} \sum_{pq}{\delta_{pq}}$ (average of dissimilarities).

Another related criterion is the S-Stress criterion, which involves squares of dissim\-ilarities and distances:
% Editor: Do you mean the Stress criterion, instead of the SSTRESS criterion?
\[
\sigma_{SSTRESS}({\X})=\sum_{s,t \in \oneton} { \left(\mathcal{D}_{st}^2({\X})-\delta_{st}^2\right)^2  }.
\]


\subsubsection{Sammon Mapping Criterion}
This is a specific case of the Stress criterion in which the weights $\{w_{st},\, s,t \in \oneton\}$ are set to be $\delta_{st}^{-1} \left[\sum_{k<l}\delta_{kl} \right]^{-1}$. These weights normalize the squared discrepancies in the stress criterion by the magnitude of the original dissimilarities so that the discrepancy terms for the larger dissimilarities do not dominate the optimization of the criterion function. As a result, small $\delta_{st}$ are preserved just as well as large $\delta_{st}$.

\subsection{Ordinal (Nonmetric) MDS}
 For Nonmetric MDS, $f(\cdot)$  is allowed to be any monotonic transformation. Specifically, in psychometric applications of MDS, the assumption that the dissimilarities are a scaled-shifted version of the ``true''  dissimilarity is an unwarranted assumption. Even, the existence of a ``true'' dissimilarity is questionable. Even if the dissimilarities are physical distances, humans tend to have biased estimates of those distances \cite{Trosset1998}. (i.e. long distances are usually underestimated.) The Nonmetric variant of MDS is also called ``ordinal'', because what is preserved is the rank of dissimilarities, not their magnitude.

\subsection{Classical MDS and the Strain Criterion}

An $n \times n$ matrix $\Delta = \left[\delta_{st}\right]$ is a distance matrix iff
\begin{itemize}
\item $\delta_{st} = \delta_{ts} , \onespace \, \forall s,t \in \oneton$ ,
\item $\delta_{ss}=0, \, \forall s \in \oneton$,
\item $\delta_{st} > 0 ,\onespace \, \forall s,t \in \oneton, s \neq t$ and 
\item if it obeys the triangle inequality $\delta_{sr}+\delta_{rt} \geq \delta_{st}$ for any triple $s,r,t \in \oneton$.
\end{itemize}
$\Delta$ is Euclidean if there exists a configuration of points $\bm{x}_{i} \in {\R}^d$ such that for any pair $s,t \in \oneton$,    $\delta_{st}=d(\bm{x}_{s},\bm{x}_{t})$.

Consider the case in which $\Delta$ is Euclidean. %and the embedding dimension $d$ is known.
Note that if $\{\bm{x}_{i}\},\, i \in \oneton  $ satisfy $\delta_{st}=d(\bm{x}_{s},\bm{x}_{t})$ for any pair $(s,t)$, then, for any constant vector $\bm{u}$ and any rotation/reflection matrix $R$, the same group of points transformed using $R$ and   $\bm{u}$, i.e.  $\{ R\bm{x}_{i}+\bm{u}\}$, also satisfy the same distance constraints. To remove the translational ambiguity, we set   $\sum_{i=1}^{n}{\bm{x}_{i}}$  to $\bm{0}$. How can we recover the original configuration   $\{\bm{x}_{i}, i \in \oneton \}$ from $\Delta$ (perhaps up to rotation/reflection)?

The relation between the entries of $\Delta$ and $\{\bm{x}_{i}\}$ can be written as
\begin{equation}
\delta_{st}^{2}= d (\bm{x}_{s},\bm{x}_{t})^2 = \| \bm{x}_{s}   \|^2 + \| \bm{x}_{t}   \|^2   -   2  {\bm{x}_{s} \cdot  \bm{x}_{t} }
\label{eq:diss_wrt_embed_coords}
\end{equation} for $s,t \in \oneton$.
Summing \eqref{eq:diss_wrt_embed_coords} over $s$, over $t$, and then over $s$ and $t$, we obtain the following identities
\begin{align}
\label{eq:diss_wrt_embed_coords_2_1}
\sum_{s'}{ \delta_{s't}^2} &= \sum_{s'}{\| \bm{x}_{s'}   \|^2} + n \|\bm{x}_{t}   \|^2 -2 \sum_{s'}{ \bm{x}_{s'}   \cdot\bm{x}_{t}    } \quad t \in \oneton \\
\label{eq:diss_wrt_embed_coords_2_2}
\sum_{t'}{ \delta_{st'}^2} &= \sum_{t'}{\| \bm{x}_{t'}   \|^2} + n\|\bm{x}_{s}   \|^2  -2 \sum_{t'}{ \bm{x}_{s}   \cdot\bm{x}_{t'}   } \quad s \in \oneton \\
\label{eq:diss_wrt_embed_coords_2_3}
\sum_{s',t'}{ \delta_{s't'}^2} &= 2n\sum_{t'}{\| \bm{x}_{t'}   \|^2}  -2\sum_{s'}{ \bm{x}_{s'} } \cdot \sum_{t'}{   \bm{x}_{t'} }
\end{align}
.

Dividing each equality by $\frac{1}{n}$ , $\frac{1}{n}$ 
and $\frac{1}{n^2}$, respectively, and using the fact that  $\sum_{i=1}^{n}{\bm{x}_{i}}=\bm{0}$,
\begin{align}
\label{eq:diss_wrt_embed_coords_3_1}
\frac{1}{n}\sum_{s'}{ \delta_{s't}^2} &= \frac{1}{n}\sum_{s'}{\|\bm{x}_{s'}   \|^2 } +  \|\bm{x}_{t}   \|^2 \quad   t \in \oneton \\
\label{eq:diss_wrt_embed_coords_3_2}
\frac{1}{n}\sum_{t'}{ \delta_{st'}^2} &= \frac{1}{n}\sum_{t'}{\|\bm{x}_{t'}   \|^2 }+  \|\bm{x}_{s}   \|^2 \quad   s \in \oneton \\
\label{eq:diss_wrt_embed_coords_3_3}
\frac{1}{n^2}\sum_{s',t'}{ \delta_{s't'}^2} &= \frac{2}{n}\sum_{t'}{\|\bm{x}_{t'}   \|^2} .
\end{align}

Reorganizing terms, we obtain
\begin{align}
\label{eq:diss_wrt_embed_coords_4_1}
\|\bm{x}_{t}   \|^2  &= \frac{1}{n}\sum_{s'}{ \delta_{s't}^2} + \frac{1}{n}\sum_{s'}{\|\bm{x}_{s'}   \|^2   } \quad t \in \oneton \\
\label{eq:diss_wrt_embed_coords_4_2}
\|\bm{x}_{s}   \|^2  &= \frac{1}{n}\sum_{t'}{ \delta_{st'}^2} + \frac{1}{n}\sum_{t'}{\|\bm{x}_{t'}   \|^2  }  \quad  s \in \oneton \\
\label{eq:diss_wrt_embed_coords_4_3}
0 &= -\frac{2}{n}\sum_{t'}{\|\bm{x}_{t'}   \|^2} +\frac{1}{n^2}\sum_{s',t'}{ \delta_{s't'}^2}.
\end{align} 


Summing the three equations, 
\eqref{eq:diss_wrt_embed_coords_4_1}, \eqref{eq:diss_wrt_embed_coords_4_2},  and  \eqref{eq:diss_wrt_embed_coords_4_3} and replacing  $\|\bm{x}_{s}   \|^2 +\|\bm{x}_{t}\|^2$  in the original equation \eqref{eq:diss_wrt_embed_coords} with this sum, we obtain 

\[
\delta_{st}^2= \frac{1}{n}\sum_{s'}{ \delta_{s't}^2} + \frac{1}{n}\sum_{s'}{\|\bm{x}_{s}}   \|^2 + \frac{1}{n}\sum_{t'}{ \delta_{st'}^2} + \frac{1}{n}\sum_{t'}{\|\bm{x}_{t'}}  \|^2  -\frac{2}{n}\sum_{t'}{\|\bm{x}_{t'}   \|^2} +\frac{1}{n^2}\sum_{s',t'}{ \delta_{s't'}^2}   -2\bm{x}_{s}   \bm{x}_{t}  
\]
for all $s,t \in \oneton$.

This expression is simplified to 
\[
\delta_{st}^2= \frac{1}{n}\sum_{s'}{ \delta_{s't}^2} +  \frac{1}{n}\sum_{t'}{ \delta_{st'}^2}  +\frac{1}{n^2}\sum_{s',t'}{ \delta_{s't'}^2}  -2\bm{x}_{s}   \bm{x}_{t} .  
\]
for all $s,t \in \oneton$.

Rearranging terms, we obtain the dot product of $\bm{x}_{s}$ and $\bm{x}_{t}$: 
\[
\bm{x}_{s}   \bm{x}_{t} = \frac{-1}{2} \{\delta_{st}^2 - \frac{1}{n}\sum_{s'}{ \delta_{s't}^2} -  \frac{1}{n}\sum_{t'}{ \delta_{st'}^2}  +\frac{1}{n^2}\sum_{s',t'}{ \delta_{s't'}^2 }      \}.
\]



Some of the sums in the above expression can be written in matrix notation as follows:
\begin{align*}
\frac{1}{n} \bm{1}^{T}\Delta^2= \frac{1}{n}\sum_{s'}{ \delta_{s't}^2}\\
\frac{1}{n} \Delta^2 \bm{1} = \frac{1}{n}\sum_{t'}{ \delta_{st'}^2}
\end{align*} where $\Delta^2$ is the $n \times n$ matrix whose entries are the squares of the respective entries of $\Delta$.

Using the above expressions and placing $\{\bm{x_i}\}$ row-wise into an $n \times d$ matrix $\X$, we can write all of the terms in matrix notation:
\begin{align*}
{\X}{\X}^T = \frac{-1}{2} \{\Delta^2 -  \frac{1}{n} \bm{1}\bm{1}^{T}\Delta^2  -  \frac{1}{n} \Delta^2 \bm{1}\bm{1}^{T} +   \frac{1}{n^2} \bm{1}\bm{1}^{T}\Delta^2 \bm{1}\bm{1}^{T}      \}.
\end{align*}

The final expression is 
\begin{align*}
{\X}{\X}^T = \frac{-1}{2} \{(I_n- \frac{1}{n}\bm{1}\bm{1}^{T})\Delta^2 (I_n- \frac{1}{n}\bm{1}\bm{1}^{T} \} .
\end{align*}

Therefore, the configuration matrix $\X$ can be recovered using a eigenvalue decomposition of $Z=\frac{-1}{2} \left( H \Delta^2 H \right) $, where $H= (I_n- \frac{1}{n}\bm{1}\bm{1}^{T})$. If  the eigenvalue decomposition of $Z$ is $Z=UDU^T$, the solution for $\X$ is $\hat{\X}=UD^{\frac{1}{2}}$  where $D^{\frac{1}{2}}$ is the entrywise square-root of $D$. Also,  $\hat{\X}={\X} R$ for some rotation matrix $R$, i.e. the solution has a rotation ambiguity. Note that because  $\Delta$ is Euclidean,  all diagonal elements of $D$ are nonnegative, and the entries of $D^{\frac{1}{2}}$ are real numbers.  

Here it is useful to make the following definition:
\begin{defn}
A $n\times n$ matrix $\Delta$ is $d$-Euclidean Distance Matrix ($d$-EDM) iff it is Euclidean for embedding dimension $d$, but not $d-1$.
\end{defn}
%$\Delta$ is $d$-EDM iff the eigenvalue matrix $D$



For dimensionality reduction, we require a lower-dimensional configuration in $\R^{d'}$, where $d'<d$ whose interpoint distances approximate    $\Delta$ (a $d$-EDM) . For classical MDS, we seek  the configuration ${\X}_{{d'}}$ that minimizes   $\|{\X}{\X}^T-{\X}_{{d'}}{\X}_{{d'}}^T\|_F^2$. This criterion function is called the ``strain'' criterion. The minimizer of the strain is found by using the $d'$   largest diagonal elements of $D$ (which are the eigenvalues of $Z$) as the diagonal elements of  $D_{{d'}}$ and the corresponding eigenvectors as the columns of $U_{{d'}}$.
 These matrices yield an $n \times d'$ configuration matrix, $\hat{\X}_{{d'}}=U_{{d'}}D_{{d'}}^{\frac{1}{2}}$. 

If $\Delta$ is not Euclidean, $Z$ is not positive semidefinite and has negative eigenvalues. In this case, these eigenvalues would be replaced by zeros. We would then proceed with choosing $d'$  largest eigenvalues of $Z$. 

Note also that the classical MDS solution is nested, i.e., if the $n \times d'$ matrix, $\X_{{d'}}$,  is the cMDS solution of the $d'$ dimensional configurations, the first $d'-1$ columns of ${\X}_{{d'}}$ comprise the solution for $d'-1$ dimensional configurations (assuming that the diagonal entries of  $D_{{d'}}$ are sorted in descending order.)


\subsection{Relationship with other embedding methods\label{MDS_SpectralEmbed}}

Note that Tang et al.\cite{MinhTrosset_SpectralEmbed} note another connection between embedding methods by showing that the spectral embedding for an unnormalized Laplacian matrix, $L$ (subject to  an appropriate scaling of dimensions), is equivalent to the classical MDS solution with the inner product matrix $Z=L^{\dag}$, where $L^{\dag}$ is the pseudo-inverse of $L$ \cite{MinhTrosset_SpectralEmbed}. Therefore, for any d-dimensional spectral embedding of the Laplacian $L$  with Laplacian Eigenmaps, there exists an omnibus dissimilarity matrix $M$, the ($d$-dimensional) cMDS embedding of which would give the same configuration.


\subsection{Effect of Perturbations}
  To determine how robust the embeddings are to error in dissimilarity measurements,  perturbation analysis is necessary. Two papers by Sibson \cite{Sibson_perturbational1979} investigate how small changes in the dissimilarity matrix change the configuration matrix obtained by classical MDS embedding. The main result in  \cite{Sibson_perturbational1979} says the following:
  
    ``Let $\mcE=\Delta^2$ for a Euclidean distance matrix $\Delta$ and $B=-{\frac{1}{2}}H\mcE H$. Let $\lambda$ be a simple eigenvalue of $B$ with unit-length eigenvector $e$. Let $F$ be a symmetric matrix whose diagonal entries are zeros. Let $\widetilde \mcE(\epsilon)= \mcE +\epsilon F + \mcO(\epsilon ^2)$ be the perturbed version of $\mcE$. Then, the eigenvalue and eigenvector of $\widetilde \mcE(\epsilon)$ (the perturbed versions of $\lambda$ and $e$ ) are  $\widetilde \lambda(\epsilon)= \lambda +\epsilon \upsilon + \mcO(\epsilon ^2)$, where $\upsilon=(-\frac{1}{2}e^TFe)$ and  $\widetilde e(\epsilon)= e +\epsilon (\frac{1}{2}\left(B-\lambda I\right)^{\dag}Fe + \frac{1}{2} \left(\lambda n\right)^{-1}(\1_N^T F e)\1_N) + \mcO(\epsilon ^2)$
    .''
    Because  $\upsilon$, the first-order perturbation of $\lambda$, is linear with respect to $F$, we can conclude 
  \[ E[\upsilon]= -\frac{1}{2}e^TE[F]e .
  \]  
  
  This result provides us with intuition about how much the eigenvalue $\lambda$ would change according to a perturbation of $\epsilon$. Specifically, the magnitude of change denoted by $\upsilon$ in the eigenvalue $\lambda$  is upperbounded  by the maximum eigenvalue of $F$. This change in the eigenvalue leads to the  scaling of  the particular dimension of the cMDS embedding  $\lambda$ corresponds to. 
\subsection{Maximum Likelihood MDS and MULTISCALE}
Various probabilistic MDS methods with specific error assumptions have been proposed.  In \cite{MacKay1989}, Mackay assumes that the ``original'' coordinates have normally distributed errors that are independent in each dimension in the embedded space (the correlated error case can be simplified to the independent error case). As a result, the individual dissimilarities have the same distribution as a weighted sum of independent chi-square-distributed  random variables. The embedding coordinates can be estimated using the maximum likelihood method. This method is implemented in an MDS program named MULTISCALE\cite{multiscale,borg+groenen:1997}.

\subsection{Three-way MDS\label{subsec:3wayMDS}}
Three-way MDS refers to  a variant of MDS that is used to analyze many different dissimilarity matrices on the same collection of $n$ objects.  The different dissimilarity matrices  can consist of dissimilarities judged by different people or different dissimilarity measures applied to the same group of observations. We say that these different dissimilarity matrices are from different ``conditions'', as mentioned in \autoref{sec:intro}, and we denote them by $\{\Delta_k, k \in \{1,\ldots,K \}\}$, where $k$ indexes the conditions. The three-way array in which the third ``way'' is indexed by $k$ can be interpreted as a tensor  and comprises  the stack of the  two-dimensional dissimilarity matrices.

There are two ways of dealing with such three-way data. One can compute a separate MDS solution for  each condition and match the configuration matrices by transformations. The second step in this two-step approach is similar to Generalized Procrustes Analysis  \autoref{sec:GenProcrustes} of \autoref{chap:PoM}. The general approach assumes that there is a common configuration $\mathbf{G}$ ($n$ points in $\R^{d}$) and $K$ $d \times d$ transformation matrices ${\mathbf{T}_k}$ such that each dissimilarity matrix $\Delta_k$ is obtained from the transformed configuration $\mathbf{G}_k =\mathbf{G}\mathbf{T}_k$. The inference problem then involves computing $\mathbf{T}_k, \quad k \in \{1, \ldots, K \}$.

Another approach mentioned in \cite{borg+groenen:1997} involves mapping the dissimilarities (or proximities) into one distance matrix (which is the idea behind Multiple Kernel Learning \ref{sec:MultiViewLearn}).


\section{Principal Components Analysis}
Let $X$ be a random vector of   $d$ dimensions and $\mu$ and $\Sigma$ be its mean vector and covariance matrix, respectively. Then, for a given dimension $d'\leq d$,  consider the successive (as $i$ goes from $1$ to $d'$) maximization of  $$Var[u_{i}^T (X-\mu)]= E[u_{i}^T (X-\mu)(X-\mu)^T u_i]=u_{i}^T\Sigma u_{i}  $$ with respect to $u_{i}$, where $u_{i}$ is a  $d$-dimensional unit vector ($u_i^Tu_i=1$)  and $u_i \perp u_j,\quad  1\leq j<i$\footnote{Due to the  orthogonality constraints, the projections of $X$  to the different dimensions are uncorrelated. \textit{i.e.}, $E[u_{i}^T (X-\mu) (X-\mu)^T u_j]=0, 1\leq j<i$.}. The maximizers $\{u_i,1\leq i \leq d'\}$ are the principal directions and the projections of $X$ via  $\{u_i\}$ yield the principal \emph{components} of $X$. These principal components capture the  maximum variance  possible from $X$, subject to the orthogonality constraints of all pairs of directions. 

Another way of understanding PCA is  considering the principal directions jointly. Consider $d' \times d$-matrix $U$ whose rows are $u_i, \, i \in \{1,\ldots,d'\}$ which forms an orthonormal basis. $U^TU$  is the projection matrix that captures the maximum variance from $X$.       That is, the elements of the random vector $UX$ are uncorrelated and have the  highest amount of ``total variance'' for any orthogonal projection of $X$. The term  ``total variance'' should be interpreted  as the sum of the variances of the variates in the principal directions which is equal to the trace of the covariance matrix of $UX$.

PCA also yields the best linear approximation of $X$ in a least-squares sense for a particular projection dimension, $d'$. The  matrix $U$, composed of the principal directions of $X$,  minimizes $Var[X - ( \Gamma^T\Gamma X)]$ with respect to $\Gamma$, when $\Gamma$ is constrained to be a $d' \times d$ matrix.

For a sample of size $n$ drawn from the same distribution as $X$, consider the sample estimates $\hat{\mu}$,  $\hat{\Sigma} $. The sample principal components  are computed by replacing the distribution parameters with the sample estimates
 \[\hat{u_i}=\argmax_{u_i^Tu_i=1, u_i \perp u_j,\,j<i} u_{i}^T \hat{\Sigma}u_i. \]
for the $i^{th}$ principal component.

Suppose $\X$ is an $n \times d$ configuration matrix, representing the sample of $X$ ($n$ \textit{i.i.d.} realizations of $X$). For simplicity of notation, suppose that the configuration is zero-centered, i.e., $\1 ^T \X  = \bm{0}$. Additionally, suppose that $\X$ has a singular value decomposition $\X= V\Lambda U^T$, where the singular values on the diagonal of $\Lambda$ are sorted in descending order and $V$ and $U$ are orthogonal $n \times n$ and $d \times d$ matrices, respectively. The PCA solution is  given by the eigenvalue decomposition of $\hat{\Sigma}$ estimate,
$\frac{1}{n} {\X}^T \X=U \left( \frac{1}{n} \Lambda^2 \right) U^T$. The columns of $U$, $\{ u_i \}$ are the principal directions. Note that we have $U$ in the SVD of $\X$, so we do not need to compute ${\X^T}\X$.

The principal coordinates are the projections of the samples of $X$ along the principal directions. For example, the first principal coordinates of the $n$ samples would be given by $u_1^T {\X}^T$. The  first $d'$ principal coordinates can be represented with the $n \times d'$ configuration matrix $\X_{d'} =  {\X}  U_{d'}$. Therefore, using the SVD decomposition of $\X$,  the principal coordinates are found to be  
\begin{align}
\X_{d'}&= V \Lambda U^T U_{d'} \\
       &= V  \Lambda \left[ \begin{array}{c}
I_{d'} \\
\bm{0}
\end{array} \right] \\
&= V \Lambda_{d'} 
\end{align}
where   $\Lambda_{d'}$  consist of the first $d'$ columns of    $\Lambda$.

\subsection{Principal Components Analysis and \\ Classical Multidimensional Scaling}
The Principal Components Analysis method  results in the same solution as classical Multidimensional Scaling (cMDS) when the dissimilarity matrix is $\Delta =\mathcal{D}(\X)$. For cMDS, the eigenvalue decomposition of the $n \times n $ matrix $\X\X^T$ is used for embedding, while for PCA, the eigenvalue decomposition of the  $d \times d $ matrix  ${\X}^T \X$ is used to compute the principal directions (assuming $\X$ is a zero-centered configuration). 

In the case of cMDS, \begin{equation}
\Delta^2=  \1_n \bm{y} +\bm{y} \1_n -2 \X \X^T \label{eq:dist_sqrd}
\end{equation} where $\bm{y}= \left(\X\X^T \right)_{(d)}$ is an n-dimensional vector that consists of  the diagonal of $\X\X^T$.
For the classical MDS procedure, given the entrywise-squared distance matrix $\Delta^2$, we compute 
\begin{equation} Z=  - \frac{1}{2} H\Delta^2 H \label{eq:tau_transform} .
\end{equation} 
Substituting $\Delta^2$ with \eqref{eq:dist_sqrd}   in \eqref{eq:tau_transform} gives $ Z= \X\X^T$, which has the same non-zero eigenvalues as the PCA solution. The MDS solution, which is computed by the eigenvalue decomposition of $Z$, is given by the $n \times d'$ configuration matrix  $V \Lambda_{d'}$. For the same embedding dimension, $d'$, the two methods would yield the same configuration of $n$ points in $d'$-dimensional space.


