\chapter{Simulations and Experiments}
\label{sec:simexp_results}
\chaptermark{Simulations and Experiments}





%Let us first investigate the effect of parameters on the empirical distribution of the test statistic, under null and alternative.
% For our Multivariate Normal and Dirichlet models, consider the signal and noise dimensions $p$ and $q$ respectively.
%  An increase in $p$ leads to the inflation of the test statistic under alternative
%  \ref{fig-stats-p}.


\section{Simulation Results\label{sec:Simulation Results}}
To compare the  different approaches, training data of matched pairs of measurements were generated according to the Dirichlet and Gaussian settings  with parameters $p,q,r,c$ \ref{chap:data_models}. Dissimilarity representations (\ref{sec:dissim_repr}) were computed from pairwise Euclidean distances of these measurements. A set of matched pairs and unmatched pairs of measurements were also generated for testing with the same distributions. Following the out-of-sample embedding of the dissimilarities test pairs (computed via by one of  P$\circ $M \ref{chap:PoM}, CCA \ref{chap:CCA}, regularized CCA \ref{sec:CCA} and JOFC \ref{chap:match_detection} approaches),  test statistics  for matched and unmatched pairs (corresponding to null and alternative hypothesis, respectively) were used to compute power values at a set of fixed type I error rate $\alpha$ values. By using the same generated data for all of the approaches, we are able to compare the performance of  different approaches either using the area under curve (AUC) measure or the statistical power at a desired $\alpha$ value.

 Additionally, to consider  relative robustness of methods , ``noisy" measurements were created from the original measurements by concatenating randomly generated independent noise vectors (\autoref{noise}).   This setting will be referred to as the ``noisy case". The magnitude of noise is controlled by the parameter $c$ in equation \eqref{eq:noise-expr}). The original setting, with $c=0$,  will be referred as the ``noiseless case".
If the magnitude of noise is small enough, and the embedding dimension is not larger than signal dimension, the embeddings provided by PCA and MDS should not be affected significantly by the noise. However  if the number of noise dimensions (controlled by the parameter $q$ in the distribution of $E_{ik}$ as defined in equation \eqref{eq:noise-expr} ) is large enough, it is expected that embeddings via  CCA  will  be affected due to spurious correlation between noisy dimensions.

We will now describe the steps for our Monte Carlo simulation in detail. Given the setting ("Gaussian","Dirichlet"),   the steps for each Monte Carlo replicate are as follows:
\begin{itemize}
\item A training set ($\mathbf{T}_{mc}$) which consists of  $n$ pairs of matched measurements is generated.  If $c=0$, the ``noiseless" data setting is being simulated and measurements are $p$-dimensional vectors, otherwise  the ``noisy" setting is being used to generate data and measurement vectors are $(p+q)$-dimensional. $ \mathbf{T}_{mc}$ = 
$\begin{array}{ccc}
        X_{11} & \ldots & X_{1K} \\
        \cdots & \cdots      & \cdots   \\ 
        X_{n1} & \ldots     & X_{nK} \\
    \end{array}
$
 where each $X_{ik}$ is a random vector of dimension $(p+q \times \I (c>0))$ and the conditional distribution  $X_{i.}|\bm{\alpha}_i  $ is specified as an appropriate Multivariate Normal or Dirichlet distribution. The data generation is also described in detail \autoref{chap:data_models}
\item  Dissimilarities are computed from $X_{ik}$, $\left[\Delta_{k}\right]_{ij}=d(X_{ik},X_{jk})$ for each condition $k$. We use Euclidean Distance for both Gaussian and Dirichlet settings.
\item Dissimilarities are embedded in  Euclidean space  via MDS. For P$\circ$M approach, the embedding is into $\R^d$ followed by a transformation from  $\R^d$ to  $\R^d$. For CCA , the embedding is into $\R^{p+q}$ , followed by projection into $\R^d$. For regularized CCA,  the embedding is into $\R^{s}$ where $s=(p+q)/2$\footnote{$s$ could be chosen as any integer between $d$ and $p+q$. This particular choice was a sensible one for the values of $p,q,d$ in our simulations.} , followed by projection into $\R^d$. The final embeddings lie in $\R^d$.   Denote this in-sample embedding configuration as   $\hat{\mathbf{T}}$. For JOFC approach, embedding is carried out with the weighted raw stress function $\sigma_{W}(\cdot)=f_{w}(D(\cdot),M)$ in equation \eqref{fid-comm-tradeoff-func} with a common weight $w$ for commensurability terms and another common weight $1-w$ for fidelity terms. We try different values of $w$ in our simulations. For P$\circ$M, CCA and regularized CCA, unweighted raw stress function ($\sigma(\cdot)$) is used as a criterion function for embedding the dissimilarities.
%These are \emph{non-uniform} and \emph{uniform}  weighting, respectively.

\item  $m$ pairs of matched   measurements are generated which are treated as out-of-sample, and 
\begin{itemize}
\item compute the dissimilarities  %$\mathbf{\Delta}^{new}={ \delta_{ik}^{new}; i=1,\ldots, n;\hspace{5pt} k=1,2}$
 between these out-of-sample  points and the points in ${\mathbf{T}_{mc}}$,  
\item  embed the OOS dissimilarities as pairs of embedded points via the OOS extension:\\
 $(\tilde{y}_1^{(1)},\tilde{y}_1^{(2)}),\ldots, (\tilde{y}_m^{(1)},\tilde{y}_m^{(2)})$, 
\item compute the test statistic $\tau$ for each pair, $\tau_i=d(\tilde{y}_i^{(1)},\tilde{y}_i^{(2)});\hspace{4pt}
i=1,\ldots,m$
\end{itemize}
 The values of the statistic $\tau={tau_i,i=1,\ldots,m}$ are used for computing  the empirical cumulative distribution function under null hypothesis. 

\item Identical steps for $m$ pairs of unmatched measurements result in the empirical cumulative distribution  function of $\tau$ under alternative hypothesis.
\item For any fixed $\alpha$ value, a critical value for the test statistic and the corresponding power is computed.
\end{itemize}



\begin{figure}
\includegraphics[scale=0.75]{MVN-FC-Tradeoff-OOS-c0_01.pdf}
\caption{Power ($\beta$) vs Type I error ($\alpha$) plot for different $w$ values for the Gaussian setting (noisy case)}
\label{fig:MVN-c001-power-alpha}
\end{figure}

\begin{figure}
\includegraphics[scale=0.75]{MVN-FC-Tradeoff-OOS-c0.pdf}
\caption{Power ($\beta$) vs Type I error ($\alpha$) plot for different $w$ values for the Gaussian setting (noiseless case)}
\label{fig:MVN-c0-power-alpha}
\end{figure}

\begin{figure}
\includegraphics[scale=0.75]{OOSMVN-power-w-c0_01.pdf}
\caption{Power ($\beta$) vs $w$ plot for different Type I error ($\alpha$) values for the Gaussian setting (noisy case)}
\label{fig:MVN-c001-power-w}
\end{figure}


\begin{figure}
\includegraphics[scale=0.75]{Dirichlet-FC-Tradeoff-OOSc0-01-n150.pdf}
\caption{Power ($\beta$) vs Type I error ($\alpha$) plot for different $w$ values for the Dirichlet setting (noisy case)}
\label{fig:Dir-c001-power-alpha}
\end{figure}

\begin{figure}
\includegraphics[scale=0.75]{Dirichlet-FC-Tradeoff-OOSc0-n150.pdf}
\caption{Power ($\beta$) vs Type I error ($\alpha$) plot for different $w$ values for the Dirichlet setting (noiseless case)}
\label{fig:Dir-c0-power-alpha}
\end{figure}

\begin{figure}
\includegraphics[scale=0.75]{OOSDirichlet-power-w-c0-01.pdf}
\caption{Power ($\beta$) vs $w$ plot for different Type I error ($\alpha$) values for the Gaussian setting (noisy case)}
\label{fig:Dir-c001-power-w}
\end{figure}

For $p=5$,$q=10$,$d=2$, and $c\in{0,0.01}$, for $n=150$ and $m=150$, the average of the power values for $nmc=150$ Monte Carlo replicates are computed at  different $\alpha$s and are plotted in Figure \ref{fig:MVN-c001-power-w} against $\alpha$ for the Gaussian setting.  
%Qualititatively similar plots for the Dirichlet setting  are not included for brevity.  
The plot in \autoref{fig:MVN-c001-power-w} shows that for different values of  $w$, $\beta$-$\alpha$ curves vary significantly.  The conclusion is that the match detection tests with JOFC embedding using specific $w$ values have better performance than other $w$ values in terms of power.  In Figure
 \ref{fig:MVN-c001-power-w},  $\beta(w)$ is plotted against $w$ for fixed values of $\alpha$. It is  interesting that the optimal value of $w$ seems to be in the range of $(0.85,1)$ for all settings, which suggests a significant emphasis on commensurability might be  critical for the match detection  task. 




\begin{comment}
\begin{figure}
\includegraphics[scale=0.75]{OOS-MVN-power-w-c0.pdf}
\caption{$\beta$ vs $w$ plot for fixed $\alpha$ values for the Gaussian setting (noiseless case)}
\label{fig:MVN-c0-beta-w}
\end{figure}


\begin{figure}
\includegraphics[scale=0.65]{OOSMVN-power-w-c001.pdf}
\caption{Power ($\beta$) vs $w$ plot for fixed Type I error ($\alpha$) values for the Gaussian setting (noisy case)}
\label{fig:MVN-c001-beta-w}
\end{figure}

\end{comment}

Note that in Figure \ref{fig:MVN-c001-power-w} for $\alpha=0.05$, $\beta_{\alpha=0.05}(w=0.99)\geq\beta_{\alpha=0.05}(w=0.5)$. However, for $\alpha=0.3$, $\beta_{\alpha=0.3}(w=0.99)\leq\beta_{\alpha=0.3}(w=0.5)$. This justifies our comment that  $w^{*}$  must be defined with respect to  the AUC measure or a specific $\alpha$ value.


\begin{comment}
\begin{figure}
\includegraphics[scale=0.75]{OOS-Dirichlet-power-w-c0.pdf}
\caption{$\beta$ vs $w$ plot for fixed $\alpha$ values for the Dirichlet setting(noiseless case)}
\label{fig:fig7}
\end{figure}

\begin{figure}
\includegraphics[scale=0.35]{OOS-Dirichlet-power-w-c0-01.pdf}
\caption{$\beta$ vs $w$ plot for fixed $\alpha$ values for the Dirichlet setting(noisy case)}
\label{fig:fig8}
\end{figure}
\end{comment}



Note that  for all of the settings, the estimate of the optimal $w^{*}$ has  higher power than $w$=0.5 (the unweighted case).
To test the statistical significance of this observation, we consider the following hypothesis test:  the null hypothesis that  $\mcH_{0}: \beta_{\alpha}({\hat{w}^*})\leq\beta_{\alpha}({w=0.5})$  is tested against the alternative $\mcH_{A}=\beta_{\alpha}({\hat{w}^*})>\beta_{\alpha}({w=0.5})$.  The least favorable null hypothesis is that  $\mcH_{0}: \beta_{\alpha}({\hat{w}^*})=\beta_{\alpha}({w=0.5})$.


McNemar's test will be used to compare the two predictors (referred to as $C_1$ and $C_2$ with $w$=0.5 and $w$=$w^*$ at a fixed $\alpha$ value.
\subsection{McNemar's Test\label{subsec:McNemarstest}}
Using previous notation,  the test statistic will be denoted by $T_a(w)$ under the alternative hypothesis and $T_0(w)$ under the null hypothesis.
For a fixed $\alpha$ value, one can compute two critical values $c(0.5)=max_l \{  P(T_0(0.5)>c)<\alpha\}$,  $c(w^*)=max_l \{  P(T_0(w_2)>c)<\alpha\}$. These critical values determine two binary classifiers, if we interpret the hypothesis testing as deciding whether a new pair is ``matched'' or not, and the test statistic as a score. Hypothesis testing is more nuanced than a binary decision problem, but for the sake of comparing two tests we can treat it as such.
%The values of the decision function that uses these critical  values, for each pair of embedded points (indexed by $i$, are  $(\tilde{y}_i^{(1)},\tilde{y}_i^{(2)}),\quad i=1,\ldots,m$.
To compare the  two statistical tests with  $w=0.5$ and $w$=$w^*$ , simulation result are used to compute $2\times 2$ contingency-tables of correct decisions and incorrect decisions made by each statistical test (or equivalently true and false classifications made by two classifiers). Let $\mathcal{D}$ denote the test dissimilarities for a new test pair, $\tau(\mathcal{D})$ the test statistic for the oos-embedding of that pair,$m_{\mathcal{D}}$ denote a binary variable whose value is $1$ if the pair is matched,$0$ otherwise. Denote decision outcome (whether the true or false decision is made) for the $i^{th}$ test pair by two binary variables $g_1^{i}$ and $g_2^{i}$,respectively.  If $g_1^{i} =1$ and $g_2^{i}=0$  for the $l^{th}$ MC replicate,  the first test made the correct decision and the second test made the incorrect decision with regard to the null and alternative hypotheses. 
\begin{align*}
g_1^{i}=\I(\I(\tau(\mathcal{D^{(i)}})>c(0.5))=m_{\mathcal{D}^{(i)}}) \textrm{for the first statistical test} \\
g_2^{i}=\I(\I(\tau(\mathcal{D^{(i)}})>c(w^*))=m_{\mathcal{D}^{(i)}}) \textrm{for the second statistical test}
\end{align*}
% McNemar's test was used to compare the two contingency tables for fixed $\alpha$. McNemar's test is a statistical test for %comparing two binary classifiers based on a 2-by-2 table of the counts of misclassifications of each. That is,
Consider the contingency table for the any Monte Carlo replicate given by $$G^{(l)}= \begin{array}{|c|c|}
      \hline
       e_{00}^{(l)} & e_{10}^{(l)}\\
      \hline
       e_{01}^{(l)} & e_{11}^{(l)}\\
      \hline
      \end{array}      $$  where  $e_{uv}^{(l)}=\sum_{i}{\I(\{g_1^{i}=u\} \&\& \{g_2^{i}=v\})}$ is equal to the number of instances at which the true hypothesis were identified  correctly ($g_1^{i}=1$) or incorrectly ($g_1^{i}=0$) by the first test, and correctly ($g_2^{i}=1$) or incorrectly ($g_2^{i}=0$) by the second test \emph{ in the $l^{th}$  MC replicate}.

Under the null  hypothesis that the two predictors have the same power at $\alpha$,
 $\Pry[\left(\{g_1^{i}=1\} \&\& \{g_2^{i}=0\}\right)]=\Pry[\left(\{g_1^{i}=0\} \&\& \{g_2^{i}=1\}\right)]$. Thus, a one-sided sign test is appropriate,  where the test statistic $e_{01}^{(l)}$ is distributed according to  the binomial distribution, $\mcB(e_{10}^{(l)}+e_{01}^{(l)},0.5)$. 

We consider simulated data with the noisy version of the Gaussian setting for this McNemar's test. The critical values $c(0.5)$ and $c(w^*)$ were computed for  allowable type I error $\alpha=0.05$ for the two tests. When comparing  the null hypothesis that  $\mcH_{0}: \beta_{\alpha}({\hat{w}^*})=\beta_{\alpha}({w=0.5})$ against the alternative $\mcH_{A}=\beta_{\alpha}({\hat{w}^*})>\beta_{\alpha}({w=0.5})$, the p-value is $p<1.09E-24$ which indicates the power using estimate of optimal $w^*$ is significantly greater than the power when using $w=0.5$.

Under the null distribution, we expect p-values for each MC replicate to be uniformly distributed. We find
 the distribution of p-values from McNemar's tests  is skewed and  we reject $\beta_{0.5}>=\beta_{w^*} $ for  55\%  of the Monte Carlo replicates.
%Insert plot here? 

\section{Effects of parameters of data model }
 Another avenue for investigation is  how the parameters of the distribution of  data such as $p$ ,$q$, $r$, $c$ and $d$ affect the results. We speculated  that as $q$, the number of   noise dimensions increases, the performance of  CCA approach would suffer, due to spurious correlations. We tested  our speculation using simulated data in the Gaussian Setting with $q=90$. The results are visualized in the  bundle of ROC curves in the Figure \ref{fig:largeq}.  Both CCA and  regularized CCA are not competitive with the JOFC approach with the appropriate $w$ values. In fact, the ROC curve for CCA is not very distinct from  random guess line. We conclude that CCA approach is not robust with respect to large number of noise dimensions, no matter what the magnitude  of the noise  is (which is controlled by the parameter $c$).

\begin{figure}
\includegraphics[scale=0.8]{MVN_JOFC_q_90_c_0_001}
\caption{Large Noise Dimension Behaviour of JOFC,P$\circ$ M and CCA approaches}
\label{fig:largeq}
\end{figure}


\section{Match Testing when the number of\\ conditions, $K$, is larger than 2\label{k_more_than_two_experiment}}


As it was mentioned, all of the approaches are generalizable to $K>2$ conditions, though an ambiguity need to be resolved. The alternative hypothesis could be  defined as the event that at least one of the K new dissimilarities are pairwise unmatched ($ H_{A1}: \exists i, j , 1\leq i < j \leq K :\bm{y}_{i} \nsim \bm{y}_{j} $ ) or it could be defined as the case that absolutely none of the K dissimilarities are pairwise matched   ($H_{A2}: \forall i, j , 1\leq i < j \leq K :\bm{y}_{i} \nsim \bm{y}_{j}$ ). We chose the alternative   $ H_{A1}$ for our simulations.
% and the sample from the alternative must be generated accordingly during Monte Carlo simulation. In the first case, an appropriate test statistic is then the maximum of the pairwise %distances between embeddings of each test K-tuple.

To adapt the P$\circ$M approach to this setting, one can use Procrustes analysis  generalized to more than two configurations. Generalized Procrustes Analysis \cite{GPCA} is described in \autoref{sec:GenProcrustes}.

We also have described generalized CCA in \autoref{sec:GenCCA}.  Of the different choices for the generalization of CCA, SUMCOR criterion was chosen.

To test whether P$\circ$M, JOFC, CCA and generalized CCA approaches are appropriate for this setting as well,
the simulations in \autoref{sec:Simulation Results} were repeated with $K$-condition data , generated by a multivariate normal model with $K=3$ conditions. 
 
 We investigate the  ``noisy" case for this setting , i.e. 
 $q$-dimensional noise vectors of magnitude $c$ were added to the matched measurements, and ``signal" vectors were multiplied by $1-c$.  
 
 The ROC curves for these simulations are shown in \ref{fig:MVN-c001-power-w-Kcond}.


\begin{figure}
\includegraphics[scale=0.95]{MVN-FC-Tradeoff-OOS-3cond.pdf}
\caption{Power ($\beta$) vs Type I error ($\alpha$) plot for different $w$ values for the Gaussian setting with $K=3$ conditions (noisy case)}
\label{fig:MVN-c001-power-w-Kcond}
\end{figure}



\section{Experiments on Wiki Data}
To test the JOFC approach with real data, a collection of articles are collected from the English Wikipedia, consisting of the
 directed 2-neighborhood of the document "Algebraic Geometry". 
   This  collection of 1382 articles and the correspondence of each article in French 
Wikipedia is our real-life dataset. It is possible to utilize both textual content of the documents and the hyperlink graph structure. The textual content of the documents is summarized by the bag-of-words model. Dissimilarities between documents  in the same language are computed by the Lin-Pantel discounted mutual information \cite{LinPantel,PantelLin}
 and cosine dissimilarity $k(x_{ik}; x_{jk}) = 1 - (x_{ik} x_{jk})/(\|x_{ik}\|_2\|x_{ik}\|_2)$. 
 The dissimilarities based on the hyperlink graph of the collection of the articles are 
 for each pair of vertices $i$ and $j$, the number of vertices one must travel to go from $i$ to $j$.  Further details about this dataset is available in \cite{Zhiliang_disparate}     
Only  dissimilarities based on the textual content will be considered in this example.
   
The exploitation task is still testing for matchedness of vertices between different conditions, in this case wiki articles that are on the same topic  in  different languages.
For hypothesis testing,   randomly held out four documents - one matched pair and one unmatched pair
 -  are used to compute empirical type I error $\alpha$ and estimate of power based on the critical value computed
  from the distribution of the test statistic for the remaining 1380 matched pairs. 
The test statistic is computed using one of the three approached mentioned  $CCA$, $P\circ M$, and $JOFC$ . 
The two sets of held-out matched pairs are embedded as $\tilde{y}_1$ and $\tilde{y}_2$, via out-of-sample
embedding, to estimate the null distribution of the test statistic $T = d(\tilde{y}_1; \tilde{y}_2)$. This allows
us to estimate critical values for any specified Type I error level. 
Then the two sets of heldout unmatched pairs are embedded as $\tilde{y}_1^{(u)}$ and $\tilde{y}_2^{(u)}$, via out-of-sample embedding. 
$T' = d(\tilde{y}_1^{(u)}; \tilde{y}_1^{(u)})$ will give us an empirical distribution of the test statistic  under the alternative hypothesis. 
And the distribution under null hypothesis and under alternative hypothesis can be used to estimate power.
Target dimensionality d is determined by the Zhu and Ghodsi  automatic dimensionality selection
method \cite{ZhuGhodsi}, resulting in d = 6 for this data set.


\begin{figure}
 \centering
\includegraphics[scale=0.8]{graphs/FidCommPaperwiki-two-cond-plot} 
\end{figure}



\section{Model Selection}
For the simulations presented up to now, the embedding dimension $d$ was set to 2. This was a convenient choice which allowed us to investigate various aspects of the JOFC and competing approaches.
However,  more care is required in selection of this parameter, since it plays such a big role in performance in general learning settings. To investigate the performance of JOFC approach as $d$ changes, we ran the usual Gaussian setting simulations. The signal dimension was set to $p=10$ and different $d=2,5,7,10,15$ values were used to test the JOFC approach.

The  plots of ROC curves in    \ref{fig:ROC-d} and  \ref{fig:ROC-d-15} shows the effect of $d$ parameter on the performance of different methods for the Gaussian setting for the noisy case.

\begin{figure}
 \centering
  \captionsetup[subfigure]{labelformat=empty}
        \begin{subfigure}[b]{0.5\textwidth}        
               \centerline{\includegraphics[width=\textwidth]{ROC-d-2.pdf}}
                \caption{d=2}
                \label{fig:ROC-d-2}
        \end{subfigure}%
         %add desired spacing between images, e. g. ~, \quad, \qquad etc. 
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.5\textwidth}           
                  \centerline{\includegraphics[width=\textwidth]{ROC-d-5.pdf}}
                \caption{d=5}
                \label{fig:ROC-d-5}
        \end{subfigure}      
        %add desired spacing between images, e. g. ~, \quad, \qquad etc.    %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{0.47\textwidth}             
               \includegraphics[width=\textwidth]{ROC-d-7.pdf}
                \caption{d=7}
                \label{fig:ROC-d-7}
        \end{subfigure}          
               \begin{subfigure}[b]{0.47\textwidth}
                \centering
               \includegraphics[width=\textwidth]{ROC-d-10.pdf}
                \caption{d=10}
                \label{fig:ROC-d-10}
        \end{subfigure}
         
        \caption{Effect of $d$ parameter on ROC plots}\label{fig:ROC-d}
        \label{fig:ROC-d}

\end{figure}

\begin{center}
\begin{figure}

                \centering
               \includegraphics[scale=0.75]{ROC-d-15.pdf}
                \caption{d=15}
                \label{fig:ROC-d-15}
       
\end{figure}
\end{center}


The results show how different approaches are sensitive to the embedding dimension. For larger $d$, CCA and regularized CCA has serious degradation in performance. We expect that this is again due to spurious correlation phenomenon, where more noise dimensions appear in the embedding as the embedding dimension increases. At the same time, the performance of  P$\circ$M and and JOFC with  $w=0.5$  improve with increasing embedding dimension and become the approaches with the best performing test statistic. JOFC with the highest $w$ values ${0.95,0.99,0.999}$  perform slightly worse while the ROC curves for the other $w$ values are more or less the same. Increasing the embedding dimension, seems to push $w^*$ towards the fidelity end ($w=0$) of the fidelity-commensurability tradeoff.
These results require more investigation, so we can  provide a rigorous explanation as to   how the embedding dimension effects the different approaches (JOFC and  P$\circ$M). Specifically, it would help how the null and alternative distributions of the test statistic for the different approaches change with $d$.



